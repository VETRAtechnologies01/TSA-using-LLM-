{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lj1rRV5zlL-",
        "outputId": "55d794de-6eb3-4f39-86ba-ee8ae33fbd14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vc72woFzrl_",
        "outputId": "8b1ac0f8-5d5d-4e87-8e49-1e89979cca64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.86.0\n",
            "    Uninstalling openai-1.86.0:\n",
            "      Successfully uninstalled openai-1.86.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCGfFVnNzxOm",
        "outputId": "8b5791c2-c8a1-45b4-f9d3-d1b9158ac0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btgWKJK5z2Uw",
        "outputId": "0a957c24-e609-46eb-960d-8ae8f6bfe38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.65\n",
            "    Uninstalling langchain-core-0.3.65:\n",
            "      Successfully uninstalled langchain-core-0.3.65\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-0.3.26 langchain-community-0.3.26 langchain-core-0.3.66 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai langchain langchain-openai python-dotenv pandas tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH7o-kjD0Cb2",
        "outputId": "3a71cfcb-40a5-4789-c856-0500f337f0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.15)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.91.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.20.1)\n",
            "Downloading langchain_openai-0.3.25-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.91.0-py3-none-any.whl (735 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m735.8/735.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed langchain-openai-0.3.25 openai-1.91.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"sk-xxx\"  # Replace with your key\n"
      ],
      "metadata": {
        "id": "HqK9lQ-s0Ij9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paECXS9Awqbn",
        "outputId": "ed41b57e-2ea9-40b0-f729-dff8c0c63f49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepseek\n",
            "  Downloading deepseek-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from deepseek) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->deepseek) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->deepseek) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->deepseek) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->deepseek) (2025.6.15)\n",
            "Downloading deepseek-1.0.0-py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: deepseek\n",
            "Successfully installed deepseek-1.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install deepseek"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NTzCru50SsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "# --- Load Keys ---\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"  # Replace if needed\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# ‚úÖ LangChain LLM (explicit API key)\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, '/content/whatsapp_chat_analysis.zip') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"‚ùå No .txt file found in ZIP archive.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in chat_data.split(\"\\n\"):\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    return messages\n",
        "\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        return json.loads(response.content)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0-100, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    try:\n",
        "        json_data = json.loads(response.content)\n",
        "        return validate_screen_time_json(json_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", response.content)\n",
        "        return response.content\n",
        "\n",
        "# Validation function for clarity_score\n",
        "def validate_screen_time_json(data):\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    if \"clarity_score\" not in data:\n",
        "        print(\"‚ö†Ô∏è 'clarity_score' missing, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "    else:\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "    return data\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            return res['choices'][0]['message']['content'].strip()\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    return response.content\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"/content/screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen)\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Pipeline failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkBdzmMsryKy",
        "outputId": "f8366a1a-5e3d-4b00-b14e-efbae070c69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "‚ùå Failed to extract chat: ZipFile requires mode 'r', 'w', 'x', or 'a'\n",
            "‚ùå Pipeline failed: [Errno 2] No such file or directory: '/content/screentime_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCxASsLr0iv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Load environment variables and OpenAI key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "api_key = openai.api_key or \"sk-your-fallback-key\"\n",
        "print(\"‚úÖ Loaded OpenAI Key:\", bool(api_key))\n",
        "\n",
        "# Initialize LangChain LLM with explicit API key\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(\"üîç Files in ZIP:\", file_list)\n",
        "            txt_files = [f for f in file_list if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"/content/whatsapp_chat_analysis.zip.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ZIP file not found: {zip_path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to extract chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages: lines NOT starting with date pattern belong to previous line\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    print(f\"‚úÖ Extracted {len(messages)} messages from chat\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    def validate_screen_time_json(data):\n",
        "      if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time data not a dict, returning None\")\n",
        "        return None\n",
        "    df[\"clarity_score\"]\n",
        "    results.get(\"clarity_score\", 50)\n",
        "\n",
        "    if \"clarity_score\" in df.columns:\n",
        "        clarity = df[\"clarity_score\"]\n",
        "    else:\n",
        "        print(\"Column 'clarity_score' not found.\")\n",
        "        clarity = None  # or some default value\n",
        "        try:\n",
        "            val = int(data[\"clarity_score\"])\n",
        "            data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        except Exception:\n",
        "            print(\"‚ö†Ô∏è 'clarity_score' invalid type, setting default 50\")\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "    # Optional: Check other keys if necessary and fill defaults or clean\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            res = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            # Normalize output\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"],\n",
        "        template=prompt_template_str\n",
        "    )\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "    return text\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" Pipeline :\", e)\n",
        "\n",
        "    import json\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n",
        "\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # Clamp clarity score between 0‚Äì100\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmLnYSLrxSH_",
        "outputId": "468ada12-1022-4700-93cd-56d1a5e9d68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded OpenAI Key: True\n",
            "üîç Files in ZIP: ['whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/1.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/10.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/11.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/12.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/13.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/14.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/15.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/16.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/17.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/18.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/19.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/2.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/20.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/21.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/3.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/4.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/5.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/6.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/7.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/8.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/9.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/A+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/Dataset.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/L+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both+sw.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/both.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_NLTK_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_neg.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_spanish_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_down.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/sentiment_text_blob_pos.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Images/translated.png', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/README.md', 'whatsapp_chat_analysis-3b04f34f20d87a7aa02ff988c1fcb892f3aa393d/Whatsapp_Analysis.ipynb']\n",
            "/content/whatsapp_chat_analysis.zip.\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            " Pipeline : '\"clarity_score\"'\n",
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2584495290>:21: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5, openai_api_key=api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import requests\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Install required packages\n",
        "!pip install python-dotenv langchain openai pandas tqdm requests\n",
        "\n",
        "# --- Setup for Google Colab ---\n",
        "# Upload your files\n",
        "print(\"Please upload your files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file names\n",
        "uploaded_files = list(uploaded.keys())\n",
        "print(\"Uploaded files:\", uploaded_files)\n",
        "\n",
        "# Set up environment variables\n",
        "os.environ[\"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\"] = \"\"  # You'll set this below\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"\"  # You'll set this below\n",
        "\n",
        "# --- API Key Setup ---\n",
        "print(\"\\nüîë API Key Setup\")\n",
        "print(\"1. Get your OpenAI API key from: https://platform.openai.com/api-keys\")\n",
        "print(\"2. Get your DeepSeek API key from their website (if using DeepSeek)\")\n",
        "\n",
        "OPENAI_API_KEY = input(\"Enter your OpenAI API key (or press Enter to skip): \").strip()\n",
        "DEEPSEEK_API_KEY = input(\"Enter your DeepSeek API key (or press Enter to skip): \").strip()\n",
        "\n",
        "if not OPENAI_API_KEY and not DEEPSEEK_API_KEY:\n",
        "    print(\"‚ùå Error: You need at least one API key to continue\")\n",
        "    exit()\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "if DEEPSEEK_API_KEY:\n",
        "    os.environ[\"DEEPSEEK_API_KEY\"] = DEEPSEEK_API_KEY\n",
        "\n",
        "# Initialize models\n",
        "llm_gpt = None\n",
        "if OPENAI_API_KEY:\n",
        "    try:\n",
        "        llm_gpt = ChatOpenAI(\n",
        "            model_name=\"gpt-3.5-turbo\",\n",
        "            temperature=0.5,\n",
        "            openai_api_key=OPENAI_API_KEY\n",
        "        )\n",
        "        print(\"‚úÖ OpenAI GPT initialized successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to initialize OpenAI: {e}\")\n",
        "\n",
        "# --- DeepSeek Helper Functions ---\n",
        "def deepseek_completion(prompt, model=\"deepseek-chat\"):\n",
        "    if not DEEPSEEK_API_KEY:\n",
        "        return \"DeepSeek API key not available\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"temperature\": 0.5\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.deepseek.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå DeepSeek API error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Model Selection Helper ---\n",
        "def get_llm_response(prompt, model=\"deepseek\"):\n",
        "    if model.lower() == \"gpt\" and llm_gpt:\n",
        "        try:\n",
        "            response = llm_gpt([HumanMessage(content=prompt)])\n",
        "            return response[0].content if isinstance(response, list) else str(response)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OpenAI error: {e}\")\n",
        "            return \"\"\n",
        "    elif model.lower() == \"deepseek\":\n",
        "        return deepseek_completion(prompt)\n",
        "    else:\n",
        "        return \"Invalid model selected\"\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"‚ùå WhatsApp file not found: {zip_path}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                return []\n",
        "            with zip_ref.open(txt_files[0]) as f:\n",
        "                try:\n",
        "                    chat_data = f.read().decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    chat_data = f.read().decode('latin1')\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error extracting chat:\", e)\n",
        "        return []\n",
        "\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = [f\"{m.group(1)}: {m.group(2)}\" for m in map(pattern.match, merged_lines) if m and \"media omitted\" not in m.group(2).lower()]\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50, model=\"deepseek\"):\n",
        "    if not messages:\n",
        "        return {\"error\": \"No messages to analyze\"}\n",
        "\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...]}\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    prompt = prompt_template.format(chat=recent)\n",
        "    response = get_llm_response(prompt, model)\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Chat JSON parse error. Raw response:\\n\", response)\n",
        "        return {\"error\": \"Failed to parse analysis\"}\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"‚ùå Screen time file not found: {csv_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        return pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df, model=\"deepseek\"):\n",
        "    if df.empty:\n",
        "        return {\"error\": \"No screen time data\"}\n",
        "\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...]}\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = prompt_template.format(data=readable)\n",
        "    response = get_llm_response(prompt, model)\n",
        "    try:\n",
        "        data = json.loads(response)\n",
        "        data[\"clarity_score\"] = max(0, min(100, int(data.get(\"clarity_score\", 50))))\n",
        "        return data\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è Screen time JSON parse error. Raw response:\\n\", response)\n",
        "        return {\"error\": \"Failed to parse analysis\"}\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df, model=\"deepseek\"):\n",
        "    if df.empty:\n",
        "        print(\"‚ùå No tweet data available\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    tweet_col = next((col for col in df.columns if col.lower() in [\"tweet\", \"text\", \"message\", \"content\"]), None)\n",
        "    if not tweet_col:\n",
        "        tweet_col = df.select_dtypes(include='object').columns[0]\n",
        "\n",
        "    def analyze_sentiment(tweet):\n",
        "        prompt = f'Tweet: \"{tweet}\"\\nClassify as one word: Positive, Negative, or Neutral.'\n",
        "        response = get_llm_response(prompt, model)\n",
        "        sentiment = response.strip().capitalize()\n",
        "        return sentiment if sentiment in [\"Positive\", \"Negative\", \"Neutral\"] else \"Neutral\"\n",
        "\n",
        "    tqdm.pandas(desc=\"Analyzing tweets\")\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment)\n",
        "    return df\n",
        "\n",
        "# --- Final Report ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df, model=\"deepseek\"):\n",
        "    if isinstance(chat_json, str) or \"error\" in chat_json:\n",
        "        chat_json = {\"error\": \"No chat analysis available\"}\n",
        "    if isinstance(screen_json, str) or \"error\" in screen_json:\n",
        "        screen_json = {\"error\": \"No screen time analysis available\"}\n",
        "    if sentiment_df.empty:\n",
        "        sentiment_summary = {\"error\": \"No sentiment analysis available\"}\n",
        "    else:\n",
        "        sentiment_summary = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "\n",
        "    prompt = \"\"\"\n",
        "You are a NeuroAI advisor. Create a comprehensive mental health report based on:\n",
        "\n",
        "1. WhatsApp Analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen Time Report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter Sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Include these sections:\n",
        "- Overall mood assessment\n",
        "- Top 3 concerns\n",
        "- Recommended media (movies/songs)\n",
        "- 3 personalized daily habits\n",
        "- Digital wellness suggestions\n",
        "\n",
        "Write in a compassionate, professional tone.\n",
        "\"\"\"\n",
        "    full_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2),\n",
        "        screen_json=json.dumps(screen_json, indent=2),\n",
        "        sentiments=json.dumps(sentiment_summary, indent=2)\n",
        "    )\n",
        "    return get_llm_response(full_prompt, model)\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "def main():\n",
        "    # Input files configuration (use the names you uploaded)\n",
        "    input_files = {\n",
        "        \"whatsapp\": None,\n",
        "        \"screen_time\": None,\n",
        "        \"twitter\": None\n",
        "    }\n",
        "\n",
        "    # Match uploaded files to expected types\n",
        "    for filename in uploaded_files:\n",
        "        if 'whatsapp' in filename.lower() or filename.endswith('.zip'):\n",
        "            input_files[\"whatsapp\"] = filename\n",
        "        elif 'screen' in filename.lower() or filename.endswith('.csv'):\n",
        "            input_files[\"screen_time\"] = filename\n",
        "        elif 'tweet' in filename.lower() or filename.endswith('.csv'):\n",
        "            input_files[\"twitter\"] = filename\n",
        "\n",
        "    # Verify we found all required files\n",
        "    missing_files = [name for name, path in input_files.items() if path is None]\n",
        "    if missing_files:\n",
        "        print(f\"‚ùå Could not identify these required files: {', '.join(missing_files)}\")\n",
        "        print(\"Please ensure your uploaded files contain these keywords in their names:\")\n",
        "        print(\"- whatsapp (or .zip) for WhatsApp chat\")\n",
        "        print(\"- screen (or .csv) for screen time data\")\n",
        "        print(\"- tweet (or .csv) for Twitter data\")\n",
        "        return\n",
        "\n",
        "    # Check available models\n",
        "    available_models = []\n",
        "    if llm_gpt:\n",
        "        available_models.append(\"gpt\")\n",
        "    if DEEPSEEK_API_KEY:\n",
        "        available_models.append(\"deepseek\")\n",
        "\n",
        "    if not available_models:\n",
        "        print(\"‚ùå No available models. Please check your API keys.\")\n",
        "        return\n",
        "\n",
        "    # Model selection\n",
        "    print(f\"\\nAvailable models: {', '.join(available_models)}\")\n",
        "    model_choice = input(\"Choose model: \").strip().lower()\n",
        "    while model_choice not in available_models:\n",
        "        print(f\"Invalid choice. Please select from: {', '.join(available_models)}\")\n",
        "        model_choice = input(\"Choose model: \").strip().lower()\n",
        "\n",
        "    try:\n",
        "        print(\"\\nüîç Starting analysis...\")\n",
        "\n",
        "        # WhatsApp analysis\n",
        "        print(\"- Analyzing WhatsApp messages...\")\n",
        "        whatsapp_messages = extract_whatsapp_messages(input_files[\"whatsapp\"])\n",
        "        whatsapp_analysis = analyze_chat(whatsapp_messages, model=model_choice)\n",
        "\n",
        "        # Screen time analysis\n",
        "        print(\"- Analyzing screen time data...\")\n",
        "        screen_df = load_screen_time(input_files[\"screen_time\"])\n",
        "        screen_analysis = analyze_screen_time(screen_df, model=model_choice)\n",
        "\n",
        "        # Twitter analysis\n",
        "        print(\"- Analyzing tweets...\")\n",
        "        tweets_df = pd.read_csv(input_files[\"twitter\"])\n",
        "        sentiment_analysis = analyze_tweets(tweets_df, model=model_choice)\n",
        "\n",
        "        # Generate final report\n",
        "        print(\"\\nüìä Generating final report...\")\n",
        "        report = synthesize_report(whatsapp_analysis, screen_analysis, sentiment_analysis, model=model_choice)\n",
        "\n",
        "        # Display the report with nice formatting\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üß† TEEN MENTAL HEALTH REPORT\".center(50))\n",
        "        print(\"=\"*50)\n",
        "        display(Markdown(report))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Pipeline failed: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Yxe-Le0k07SN",
        "outputId": "4b41384b-784a-494c-f776-1f4d059aa0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Please upload your files:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4c5c12f6-5599-4a4f-8849-6e641c3b9301\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4c5c12f6-5599-4a4f-8849-6e641c3b9301\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TSA-teen age group, whatsapp_chat&screen_time_analysis.ipynb to TSA-teen age group, whatsapp_chat&screen_time_analysis (2).ipynb\n",
            "Uploaded files: ['TSA-teen age group, whatsapp_chat&screen_time_analysis (2).ipynb']\n",
            "\n",
            "üîë API Key Setup\n",
            "1. Get your OpenAI API key from: https://platform.openai.com/api-keys\n",
            "2. Get your DeepSeek API key from their website (if using DeepSeek)\n",
            "Enter your OpenAI API key (or press Enter to skip): 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\n",
            "Enter your DeepSeek API key (or press Enter to skip): \n",
            "‚úÖ OpenAI GPT initialized successfully\n",
            "‚ùå Could not identify these required files: screen_time, twitter\n",
            "Please ensure your uploaded files contain these keywords in their names:\n",
            "- whatsapp (or .zip) for WhatsApp chat\n",
            "- screen (or .csv) for screen time data\n",
            "- tweet (or .csv) for Twitter data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas python-dotenv mistral_ai openai tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKCwdmdx3xcP",
        "outputId": "d8d5ef9d-8459-417e-a058-d3a23399339c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement mistral_ai (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for mistral_ai\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "import openai\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "llm = openai.ChatCompletion()\n",
        "\n",
        "# WhatsApp Chat Extraction\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    # Placeholder for actual extraction logic\n",
        "    return [\"User1: Hello!\", \"User2: Hi there!\", \"User1: How are you?\"]\n",
        "\n",
        "# WhatsApp Chat Analysis\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "    You are a futuristic AI therapist from 2030.\n",
        "\n",
        "    Analyze these WhatsApp messages:\n",
        "    - Emotional tone (stress, joy, anxiety)\n",
        "    - Mental clarity & decision style\n",
        "    - Mindset type: proactive, reactive, balanced\n",
        "\n",
        "    Recommend:\n",
        "    - 3 apps/habits to avoid\n",
        "    - 3 uplifting movies and songs\n",
        "    - 3 good daily mental health habits\n",
        "\n",
        "    Output ONLY in JSON format as:\n",
        "    {\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "    Chat:\n",
        "    {chat}\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(chat=recent)\n",
        "\n",
        "    response = llm.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "    )\n",
        "\n",
        "    text = response['choices'][0]['message']['content']\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"JSON parsing failed in chat analysis. Raw output:\\n\", text)\n",
        "        return text\n",
        "\n",
        "# Screen Time Analysis\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "    You are a digital wellness AI.\n",
        "\n",
        "    Analyze this screen time data:\n",
        "    - Focus vs distraction\n",
        "    - Burnout, overuse, addiction\n",
        "    - Decision fatigue signs\n",
        "\n",
        "    Recommend:\n",
        "    - Mental clarity (0-100)\n",
        "    - Avoid apps\n",
        "    - 3 inspiring movies and calming songs\n",
        "    - 3 digital detox habits\n",
        "\n",
        "    Output ONLY in JSON format as:\n",
        "    {\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "    Screen Time Data:\n",
        "    {data}\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "    )\n",
        "\n",
        "    text = response['choices'][0]['message']['content']\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "        if not isinstance(data, dict):\n",
        "            raise ValueError(\"Screen time JSON result is not a dictionary.\")\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(\"Error processing screen time analysis:\", e)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "# Twitter Sentiment Analysis\n",
        "def analyze_tweets(df):\n",
        "    print(\"Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = next((col for col in df.columns if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]), None)\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\\n\\nTweet: \\\"{tweet}\\\"\\nSentiment:\"\n",
        "        try:\n",
        "            res = llm.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0\n",
        "            )\n",
        "            sentiment = res['choices'][0]['message']['content'].strip()\n",
        "            return sentiment if sentiment.lower() in [\"positive\", \"negative\", \"neutral\"] else \"Neutral\"\n",
        "        except Exception as e:\n",
        "            print(\"Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# Final Report Synthesis\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "    You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "    Combine these:\n",
        "    1. WhatsApp analysis:\n",
        "    {chat_json}\n",
        "\n",
        "    2. Screen time report:\n",
        "    {screen_json}\n",
        "\n",
        "    3. Twitter sentiment:\n",
        "    {sentiments}\n",
        "\n",
        "    Summarize teen mental health:\n",
        "    - Mood and stress pattern\n",
        "    - Top 3 issues\n",
        "    - Mindfulness movie/song list\n",
        "    - Futuristic habit suggestions\n",
        "\n",
        "    Respond warmly and clearly.\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"chat_json\", \"screen_json\", \"sentiments\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    response = llm.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# Main Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        df_tweets = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nFinal Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Pipeline error:\", e)\n",
        "\n",
        "\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    prompt = PromptTemplate(input_variables=[\"data\"], template=prompt_template_str)\n",
        "    formatted_prompt = prompt.format(data=readable)\n",
        "\n",
        "    response = llm([HumanMessage(content=formatted_prompt)])\n",
        "    text = response[0].content if isinstance(response, list) else getattr(response, \"content\", str(response))\n",
        "\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", text)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # ‚úÖ Validation & fallback handling\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    # Clamp clarity score between 0‚Äì100\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1dchuTkdthX",
        "outputId": "0e4dc836-fd5b-40e4-e8cf-f2e133d37fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline error: '\"emotional_tone\"'\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0ai178DzG7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "# Mock Mistral client for demonstration purposes\n",
        "class MistralClient:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def generate(self, model, prompt, max_tokens):\n",
        "        # Mock response\n",
        "        return json.dumps({\n",
        "            \"emotional_tone\": \"Neutral\",\n",
        "            \"clarity\": \"Clear\",\n",
        "            \"mindset\": \"Balanced\",\n",
        "            \"avoid\": [\"app1\", \"app2\", \"app3\"],\n",
        "            \"recommend\": {\"movies\": [\"Movie1\", \"Movie2\", \"Movie3\"], \"songs\": [\"Song1\", \"Song2\", \"Song3\"]},\n",
        "            \"habits\": [\"Habit1\", \"Habit2\", \"Habit3\"]\n",
        "        })\n",
        "\n",
        "mistral_client = MistralClient(api_key=mistral_api_key)\n",
        "\n",
        "# Example function to analyze chat using Mistral\n",
        "def analyze_chat_with_mistral(messages, n=50):\n",
        "    # Ensure that messages is a list and has elements\n",
        "    if not isinstance(messages, list) or not messages:\n",
        "        return {\"error\": \"Invalid messages input\"}\n",
        "\n",
        "    # Join the last n messages\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_template_str = \"\"\"\n",
        "    You are a futuristic AI therapist from 2030.\n",
        "\n",
        "    Analyze these WhatsApp messages:\n",
        "    - Emotional tone (stress, joy, anxiety)\n",
        "    - Mental clarity & decision style\n",
        "    - Mindset type: proactive, reactive, balanced\n",
        "\n",
        "    Recommend:\n",
        "    - 3 apps/habits to avoid\n",
        "    - 3 uplifting movies and songs\n",
        "    - 3 good daily mental health habits\n",
        "\n",
        "    Output ONLY in JSON format as:\n",
        "    {{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...] }}\n",
        "\n",
        "    Chat:\n",
        "    {chat}\n",
        "    \"\"\"\n",
        "\n",
        "    # Format the prompt string with the recent messages\n",
        "    prompt = prompt_template_str.format(chat=recent)\n",
        "\n",
        "    # Hypothetical call to Mistral's API\n",
        "    response = mistral_client.generate(\n",
        "        model=\"mistral-model\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"JSON parsing failed in chat analysis. Raw output:\\n\", response)\n",
        "        return {\"error\": \"Failed to parse JSON response\"}\n",
        "\n",
        "\n",
        "# Example usage\n",
        "messages = [\"User1: Hello!\", \"User2: Hi there!\", \"User1: How are you?\"]\n",
        "analysis_result = analyze_chat_with_mistral(messages)\n",
        "print(analysis_result)\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(data=readable)\n",
        "\n",
        "    # Mock response for demonstration\n",
        "    mock_response = json.dumps({\n",
        "        \"clarity_score\": 75,\n",
        "        \"fatigue\": \"Low\",\n",
        "        \"avoid_apps\": [\"App1\", \"App2\", \"App3\"],\n",
        "        \"recommend\": {\"movies\": [\"Movie1\", \"Movie2\", \"Movie3\"], \"songs\": [\"Song1\", \"Song2\", \"Song3\"]},\n",
        "        \"habits\": [\"Habit1\", \"Habit2\", \"Habit3\"]\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        data = json.loads(mock_response)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è JSON parsing failed in screen time analysis. Raw output:\\n\", mock_response)\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        print(\"‚ö†Ô∏è Screen time JSON result is not a dictionary.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "    try:\n",
        "        val = int(data.get(\"clarity_score\", 50))\n",
        "        data[\"clarity_score\"] = max(0, min(100, val))\n",
        "    except Exception:\n",
        "        print(\"‚ö†Ô∏è Invalid clarity_score, setting default 50\")\n",
        "        data[\"clarity_score\"] = 50\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- Twitter Sentiment Analysis ---\n",
        "def analyze_tweets(df):\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "        print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "\n",
        "    def analyze_sentiment_llm(tweet):\n",
        "        prompt = f\"\"\"\n",
        "You are a sentiment expert. Classify the tweet as one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "Sentiment:\"\"\"\n",
        "        try:\n",
        "            # Mock response for demonstration\n",
        "            sentiment = \"Neutral\"\n",
        "            if sentiment.lower() not in [\"positive\", \"negative\", \"neutral\"]:\n",
        "                return \"Neutral\"\n",
        "            return sentiment\n",
        "        except Exception as e:\n",
        "            print(\"‚ö†Ô∏è Error analyzing tweet:\", e)\n",
        "            return \"Error\"\n",
        "\n",
        "    tqdm.pandas()\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_sentiment_llm)\n",
        "    return df\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json, sentiment_df):\n",
        "    sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "    sentiment_summary = f\"Sentiment counts: {sentiment_counts}\"\n",
        "\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "3. Twitter sentiment:\n",
        "{sentiments}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json),\n",
        "        screen_json=json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json),\n",
        "        sentiments=sentiment_summary\n",
        "    )\n",
        "\n",
        "    # Mock response for demonstration\n",
        "    mock_response = \"Final mental health summary based on the provided data.\"\n",
        "\n",
        "    return mock_response\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # Mock function for demonstration\n",
        "        def extract_whatsapp_messages(file_path):\n",
        "            return [\"User1: Hello!\", \"User2: Hi there!\", \"User1: How are you?\"]\n",
        "\n",
        "        # 1. WhatsApp\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat_with_mistral(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "        # 2. Screen Time\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "        # 3. Twitter Sentiment\n",
        "        df_tweets = pd.DataFrame({\"tweet\": [\"I love this!\", \"Feeling great today\", \"Not sure about this\"]})\n",
        "        sentiment_df = analyze_tweets(df_tweets)\n",
        "\n",
        "        # 4. Final Mental Health Report\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Pipeline Error:\", e)\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n",
        "\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwOoN6CbzLNj",
        "outputId": "a00cfe3e-5fc2-4d87-a7ad-853348505c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'emotional_tone': 'Neutral', 'clarity': 'Clear', 'mindset': 'Balanced', 'avoid': ['app1', 'app2', 'app3'], 'recommend': {'movies': ['Movie1', 'Movie2', 'Movie3'], 'songs': ['Song1', 'Song2', 'Song3']}, 'habits': ['Habit1', 'Habit2', 'Habit3']}\n",
            "‚úÖ Loaded screen time data: 200 rows, 5 columns\n",
            "Pipeline Error: '\"clarity_score\"'\n",
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJuh4GK7bFqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import anthropic\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "# Claude client setup\n",
        "claude_client = anthropic.Anthropic(api_key=claude_api_key)\n",
        "\n",
        "# Function to analyze chat using Claude\n",
        "def analyze_chat_with_claude(messages, n=50):\n",
        "    \"\"\"Analyze WhatsApp messages using Claude AI\"\"\"\n",
        "    if not isinstance(messages, list) or not messages:\n",
        "        return {\"error\": \"Invalid messages input\"}\n",
        "\n",
        "    # Join the last n messages\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_template = f\"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages for a teenager's mental health:\n",
        "- Emotional tone (stress, joy, anxiety, neutral)\n",
        "- Mental clarity & decision style (clear, confused, impulsive)\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid for better mental health\n",
        "- 3 uplifting movies and 3 calming songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Chat messages:\n",
        "{recent}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract the JSON from Claude's response\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Try to find JSON in the response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            return json.loads(response_text)\n",
        "        else:\n",
        "            # Look for JSON within the response\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                return json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing chat with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze chat: {str(e)}\",\n",
        "            \"emotional_tone\": \"Unknown\",\n",
        "            \"clarity\": \"Unknown\",\n",
        "            \"mindset\": \"Unknown\",\n",
        "            \"avoid\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Screen Time Analysis with Claude\n",
        "def load_screen_time(csv_path):\n",
        "    \"\"\"Load screen time data from CSV\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time_with_claude(df):\n",
        "    \"\"\"Analyze screen time data using Claude AI\"\"\"\n",
        "    if df.empty:\n",
        "        return {\"error\": \"No screen time data provided\"}\n",
        "\n",
        "    # Convert dataframe to readable format (limit rows for API)\n",
        "    sample_data = df.head(20).to_string(index=False)\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a digital wellness AI expert.\n",
        "\n",
        "Analyze this screen time data for a teenager:\n",
        "- Assess focus vs distraction patterns\n",
        "- Identify signs of digital burnout, overuse, or addiction\n",
        "- Look for decision fatigue indicators\n",
        "\n",
        "Provide recommendations:\n",
        "- Mental clarity score (0-100, where 100 is excellent focus)\n",
        "- Apps to avoid or limit\n",
        "- 3 inspiring movies and 3 calming songs for digital detox\n",
        "- 3 practical digital detox habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Screen Time Data:\n",
        "{sample_data}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Extract JSON from response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            data = json.loads(response_text)\n",
        "        else:\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                data = json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "        # Validate and clean clarity_score\n",
        "        try:\n",
        "            clarity_score = int(data.get(\"clarity_score\", 50))\n",
        "            data[\"clarity_score\"] = max(0, min(100, clarity_score))\n",
        "        except:\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing screen time with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze screen time: {str(e)}\",\n",
        "            \"clarity_score\": 50,\n",
        "            \"fatigue\": \"Unknown\",\n",
        "            \"avoid_apps\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Twitter Sentiment Analysis with Claude\n",
        "def analyze_tweet_sentiment_claude(tweet):\n",
        "    \"\"\"Analyze sentiment of a single tweet using Claude\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Analyze the sentiment of this tweet. Respond with exactly one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",  # Using Haiku for faster sentiment analysis\n",
        "            max_tokens=10,\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        sentiment = response.content[0].text.strip()\n",
        "\n",
        "        # Validate sentiment\n",
        "        valid_sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
        "        if sentiment.lower() in valid_sentiments:\n",
        "            return sentiment.capitalize()\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error analyzing tweet sentiment: {e}\")\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_tweets_with_claude(df):\n",
        "    \"\"\"Analyze sentiment for all tweets in dataframe\"\"\"\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "\n",
        "    # Find tweet column\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        if not string_cols.empty:\n",
        "            tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "            print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "        else:\n",
        "            print(\"‚ùå No suitable text column found\")\n",
        "            return df\n",
        "\n",
        "    # Analyze sentiments with progress bar\n",
        "    tqdm.pandas(desc=\"Analyzing tweet sentiments\")\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_tweet_sentiment_claude)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Final Report Synthesis with Claude\n",
        "def synthesize_report_with_claude(chat_json, screen_json, sentiment_df):\n",
        "    \"\"\"Generate final mental health report using Claude\"\"\"\n",
        "\n",
        "    # Get sentiment summary\n",
        "    if \"sentiment\" in sentiment_df.columns:\n",
        "        sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "        sentiment_summary = f\"Sentiment distribution: {sentiment_counts}\"\n",
        "    else:\n",
        "        sentiment_summary = \"No sentiment data available\"\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a compassionate NeuroAI mental health advisor from the future.\n",
        "\n",
        "Synthesize these data sources to create a comprehensive teen mental health summary:\n",
        "\n",
        "1. WhatsApp Chat Analysis:\n",
        "{json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json)}\n",
        "\n",
        "2. Screen Time Analysis:\n",
        "{json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json)}\n",
        "\n",
        "3. Social Media Sentiment Analysis:\n",
        "{sentiment_summary}\n",
        "\n",
        "Please provide a warm, supportive, and actionable summary that includes:\n",
        "- Overall mood and stress patterns\n",
        "- Top 3 mental health concerns identified\n",
        "- Curated list of mindfulness movies and calming songs\n",
        "- 5 futuristic but practical daily habits for better mental wellness\n",
        "\n",
        "Write in a caring, encouraging tone as if speaking directly to the teenager.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.4,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating final report: {e}\")\n",
        "        return f\"Unable to generate comprehensive report due to error: {str(e)}\"\n",
        "\n",
        "# Mock function for WhatsApp extraction (replace with actual implementation)\n",
        "def extract_whatsapp_messages(file_path):\n",
        "    \"\"\"Mock function - replace with actual WhatsApp extraction logic\"\"\"\n",
        "    return [\n",
        "        \"User1: Feeling so stressed about exams üò∞\",\n",
        "        \"User2: Same here, can't sleep properly\",\n",
        "        \"User1: Social media is making me feel worse\",\n",
        "        \"User2: Maybe we should take a break from it\",\n",
        "        \"User1: Yeah, let's try some offline activities\",\n",
        "        \"User2: Good idea! Want to go for a walk?\"\n",
        "    ]\n",
        "\n",
        "# MAIN PIPELINE\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"üß† Starting Mental Health Analysis with Claude AI...\")\n",
        "\n",
        "    # File paths (update these to your actual file paths)\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp Analysis\n",
        "        print(\"\\nüì± Analyzing WhatsApp messages...\")\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat_with_claude(chat_msgs) if chat_msgs else {\"error\": \"No messages found\"}\n",
        "        print(\"‚úÖ WhatsApp analysis complete\")\n",
        "\n",
        "        # 2. Screen Time Analysis\n",
        "        print(\"\\n‚è∞ Analyzing screen time data...\")\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time_with_claude(df_screen) if not df_screen.empty else {\"error\": \"No screen time data\"}\n",
        "        print(\"‚úÖ Screen time analysis complete\")\n",
        "\n",
        "        # 3. Twitter Sentiment Analysis\n",
        "        print(\"\\nüê¶ Analyzing social media sentiment...\")\n",
        "        try:\n",
        "            df_tweets = pd.read_csv(twitter_file)\n",
        "        except:\n",
        "            # Create sample data if file doesn't exist\n",
        "            df_tweets = pd.DataFrame({\n",
        "                \"tweet\": [\n",
        "                    \"Having such a great day! üòä\",\n",
        "                    \"Feeling overwhelmed with everything...\",\n",
        "                    \"Just finished a good workout, feeling energized!\",\n",
        "                    \"Can't focus on anything today üòî\",\n",
        "                    \"Grateful for my friends and family ‚ù§Ô∏è\"\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        sentiment_df = analyze_tweets_with_claude(df_tweets)\n",
        "        print(\"‚úÖ Sentiment analysis complete\")\n",
        "\n",
        "        # 4. Generate Final Report\n",
        "        print(\"\\nüìã Generating comprehensive mental health report...\")\n",
        "        final_report = synthesize_report_with_claude(chat_result, screen_result, sentiment_df)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üß† COMPREHENSIVE MENTAL HEALTH ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_report)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save results to JSON file\n",
        "        results = {\n",
        "            \"chat_analysis\": chat_result,\n",
        "            \"screen_time_analysis\": screen_result,\n",
        "            \"sentiment_analysis\": sentiment_df.to_dict('records') if not sentiment_df.empty else [],\n",
        "            \"final_report\": final_report,\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "\n",
        "        with open(\"mental_health_analysis_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"\\nüíæ Results saved to 'mental_health_analysis_results.json'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline Error: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if API key is available\n",
        "    if not claude_api_key:\n",
        "        print(\"‚ùå Please set your ANTHROPIC_API_KEY in your .env file\")\n",
        "        print(\"You can get your API key from: https://console.anthropic.com/\")\n",
        "    else:\n",
        "        main()\n",
        "\n",
        "# Example of how to use individual functions\n",
        "def example_usage():\n",
        "    \"\"\"Example of how to use the individual analysis functions\"\"\"\n",
        "\n",
        "    # Example chat messages\n",
        "    sample_messages = [\n",
        "        \"Feeling really anxious about school lately\",\n",
        "        \"Social media is making me feel worse about myself\",\n",
        "        \"Maybe I should take a break from Instagram\",\n",
        "        \"My sleep schedule is completely messed up\",\n",
        "        \"I want to feel better but don't know how\"\n",
        "    ]\n",
        "\n",
        "    # Analyze chat\n",
        "    chat_analysis = analyze_chat_with_claude(sample_messages)\n",
        "    print(\"Chat Analysis:\", json.dumps(chat_analysis, indent=2))\n",
        "\n",
        "    # Create sample screen time data\n",
        "    sample_screen_time = pd.DataFrame({\n",
        "        'App': ['Instagram', 'TikTok', 'YouTube', 'WhatsApp', 'Games'],\n",
        "        'Hours': [3.5, 2.8, 4.2, 1.5, 2.1],\n",
        "        'Pickups': [45, 38, 25, 60, 15]\n",
        "    })\n",
        "\n",
        "    # Analyze screen time\n",
        "    screen_analysis = analyze_screen_time_with_claude(sample_screen_time)\n",
        "    print(\"Screen Time Analysis:\", json.dumps(screen_analysis, indent=2))\n",
        "\n",
        "# Uncomment to run example\n",
        "# example_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5bGhlfjbFXo",
        "outputId": "69d831fc-c5ea-4790-d6dd-49f79e2822c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Please set your ANTHROPIC_API_KEY in your .env file\n",
            "You can get your API key from: https://console.anthropic.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import anthropic\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "claude_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "# Claude client setup\n",
        "claude_client = anthropic.Anthropic(api_key=claude_api_key)\n",
        "\n",
        "# Function to analyze chat using Claude\n",
        "def analyze_chat_with_claude(messages, n=50):\n",
        "    \"\"\"Analyze WhatsApp messages using Claude AI\"\"\"\n",
        "    if not isinstance(messages, list) or not messages:\n",
        "        return {\"error\": \"Invalid messages input\"}\n",
        "\n",
        "    # Join the last n messages\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_template = f\"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages for a teenager's mental health:\n",
        "- Emotional tone (stress, joy, anxiety, neutral)\n",
        "- Mental clarity & decision style (clear, confused, impulsive)\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid for better mental health\n",
        "- 3 uplifting movies and 3 calming songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Chat messages:\n",
        "{recent}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract the JSON from Claude's response\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Try to find JSON in the response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            return json.loads(response_text)\n",
        "        else:\n",
        "            # Look for JSON within the response\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                return json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing chat with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze chat: {str(e)}\",\n",
        "            \"emotional_tone\": \"Unknown\",\n",
        "            \"clarity\": \"Unknown\",\n",
        "            \"mindset\": \"Unknown\",\n",
        "            \"avoid\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Screen Time Analysis with Claude\n",
        "def load_screen_time(csv_path):\n",
        "    \"\"\"Load screen time data from CSV\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time_with_claude(df):\n",
        "    \"\"\"Analyze screen time data using Claude AI\"\"\"\n",
        "    if df.empty:\n",
        "        return {\"error\": \"No screen time data provided\"}\n",
        "\n",
        "    # Convert dataframe to readable format (limit rows for API)\n",
        "    sample_data = df.head(20).to_string(index=False)\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a digital wellness AI expert.\n",
        "\n",
        "Analyze this screen time data for a teenager:\n",
        "- Assess focus vs distraction patterns\n",
        "- Identify signs of digital burnout, overuse, or addiction\n",
        "- Look for decision fatigue indicators\n",
        "\n",
        "Provide recommendations:\n",
        "- Mental clarity score (0-100, where 100 is excellent focus)\n",
        "- Apps to avoid or limit\n",
        "- 3 inspiring movies and 3 calming songs for digital detox\n",
        "- 3 practical digital detox habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Screen Time Data:\n",
        "{sample_data}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Extract JSON from response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            data = json.loads(response_text)\n",
        "        else:\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                data = json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "        # Validate and clean clarity_score\n",
        "        try:\n",
        "            clarity_score = int(data.get(\"clarity_score\", 50))\n",
        "            data[\"clarity_score\"] = max(0, min(100, clarity_score))\n",
        "        except:\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing screen time with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze screen time: {str(e)}\",\n",
        "            \"clarity_score\": 50,\n",
        "            \"fatigue\": \"Unknown\",\n",
        "            \"avoid_apps\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Twitter Sentiment Analysis with Claude\n",
        "def analyze_tweet_sentiment_claude(tweet):\n",
        "    \"\"\"Analyze sentiment of a single tweet using Claude\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Analyze the sentiment of this tweet. Respond with exactly one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",  # Using Haiku for faster sentiment analysis\n",
        "            max_tokens=10,\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        sentiment = response.content[0].text.strip()\n",
        "\n",
        "        # Validate sentiment\n",
        "        valid_sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
        "        if sentiment.lower() in valid_sentiments:\n",
        "            return sentiment.capitalize()\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error analyzing tweet sentiment: {e}\")\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_tweets_with_claude(df):\n",
        "    \"\"\"Analyze sentiment for all tweets in dataframe\"\"\"\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "\n",
        "    # Find tweet column\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        if not string_cols.empty:\n",
        "            tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "            print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "        else:\n",
        "            print(\"‚ùå No suitable text column found\")\n",
        "            return df\n",
        "\n",
        "    # Analyze sentiments with progress bar\n",
        "    tqdm.pandas(desc=\"Analyzing tweet sentiments\")\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_tweet_sentiment_claude)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Final Report Synthesis with Claude\n",
        "def synthesize_report_with_claude(chat_json, screen_json, sentiment_df):\n",
        "    \"\"\"Generate final mental health report using Claude\"\"\"\n",
        "\n",
        "    # Get sentiment summary\n",
        "    if \"sentiment\" in sentiment_df.columns:\n",
        "        sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "        sentiment_summary = f\"Sentiment distribution: {sentiment_counts}\"\n",
        "    else:\n",
        "        sentiment_summary = \"No sentiment data available\"\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a compassionate NeuroAI mental health advisor from the future.\n",
        "\n",
        "Synthesize these data sources to create a comprehensive teen mental health summary:\n",
        "\n",
        "1. WhatsApp Chat Analysis:\n",
        "{json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json)}\n",
        "\n",
        "2. Screen Time Analysis:\n",
        "{json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json)}\n",
        "\n",
        "3. Social Media Sentiment Analysis:\n",
        "{sentiment_summary}\n",
        "\n",
        "Please provide a warm, supportive, and actionable summary that includes:\n",
        "- Overall mood and stress patterns\n",
        "- Top 3 mental health concerns identified\n",
        "- Curated list of mindfulness movies and calming songs\n",
        "- 5 futuristic but practical daily habits for better mental wellness\n",
        "\n",
        "Write in a caring, encouraging tone as if speaking directly to the teenager.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.4,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating final report: {e}\")\n",
        "        return f\"Unable to generate comprehensive report due to error: {str(e)}\"\n",
        "\n",
        "# Mock function for WhatsApp extraction (replace with actual implementation)\n",
        "def extract_whatsapp_messages(file_path):\n",
        "    \"\"\"Mock function - replace with actual WhatsApp extraction logic\"\"\"\n",
        "    return [\n",
        "        \"User1: Feeling so stressed about exams üò∞\",\n",
        "        \"User2: Same here, can't sleep properly\",\n",
        "        \"User1: Social media is making me feel worse\",\n",
        "        \"User2: Maybe we should take a break from it\",\n",
        "        \"User1: Yeah, let's try some offline activities\",\n",
        "        \"User2: Good idea! Want to go for a walk?\"\n",
        "    ]\n",
        "\n",
        "# MAIN PIPELINE\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"üß† Starting Mental Health Analysis with Claude AI...\")\n",
        "\n",
        "    # File paths (update these to your actual file paths)\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp Analysis\n",
        "        print(\"\\nüì± Analyzing WhatsApp messages...\")\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat_with_claude(chat_msgs) if chat_msgs else {\"error\": \"No messages found\"}\n",
        "        print(\"‚úÖ WhatsApp analysis complete\")\n",
        "\n",
        "        # 2. Screen Time Analysis\n",
        "        print(\"\\n‚è∞ Analyzing screen time data...\")\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time_with_claude(df_screen) if not df_screen.empty else {\"error\": \"No screen time data\"}\n",
        "        print(\"‚úÖ Screen time analysis complete\")\n",
        "\n",
        "        # 3. Twitter Sentiment Analysis\n",
        "        print(\"\\nüê¶ Analyzing social media sentiment...\")\n",
        "        try:\n",
        "            df_tweets = pd.read_csv(twitter_file)\n",
        "        except:\n",
        "            # Create sample data if file doesn't exist\n",
        "            df_tweets = pd.DataFrame({\n",
        "                \"tweet\": [\n",
        "                    \"Having such a great day! üòä\",\n",
        "                    \"Feeling overwhelmed with everything...\",\n",
        "                    \"Just finished a good workout, feeling energized!\",\n",
        "                    \"Can't focus on anything today üòî\",\n",
        "                    \"Grateful for my friends and family ‚ù§Ô∏è\"\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        sentiment_df = analyze_tweets_with_claude(df_tweets)\n",
        "        print(\"‚úÖ Sentiment analysis complete\")\n",
        "\n",
        "        # 4. Generate Final Report\n",
        "        print(\"\\nüìã Generating comprehensive mental health report...\")\n",
        "        final_report = synthesize_report_with_claude(chat_result, screen_result, sentiment_df)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üß† COMPREHENSIVE MENTAL HEALTH ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_report)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save results to JSON file\n",
        "        results = {\n",
        "            \"chat_analysis\": chat_result,\n",
        "            \"screen_time_analysis\": screen_result,\n",
        "            \"sentiment_analysis\": sentiment_df.to_dict('records') if not sentiment_df.empty else [],\n",
        "            \"final_report\": final_report,\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "\n",
        "        with open(\"mental_health_analysis_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"\\nüíæ Results saved to 'mental_health_analysis_results.json'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline Error: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if API key is available\n",
        "    if not claude_api_key:\n",
        "        print(\"‚ùå Please set your ANTHROPIC_API_KEY in your .env file\")\n",
        "        print(\"You can get your API key from: https://console.anthropic.com/\")\n",
        "    else:\n",
        "        main()\n",
        "\n",
        "# Example of how to use individual functions\n",
        "def example_usage():\n",
        "    \"\"\"Example of how to use the individual analysis functions\"\"\"\n",
        "\n",
        "    # Example chat messages\n",
        "    sample_messages = [\n",
        "        \"Feeling really anxious about school lately\",\n",
        "        \"Social media is making me feel worse about myself\",\n",
        "        \"Maybe I should take a break from Instagram\",\n",
        "        \"My sleep schedule is completely messed up\",\n",
        "        \"I want to feel better but don't know how\"\n",
        "    ]\n",
        "\n",
        "    # Analyze chat\n",
        "    chat_analysis = analyze_chat_with_claude(sample_messages)\n",
        "    print(\"Chat Analysis:\", json.dumps(chat_analysis, indent=2))\n",
        "\n",
        "    # Create sample screen time data\n",
        "    sample_screen_time = pd.DataFrame({\n",
        "        'App': ['Instagram', 'TikTok', 'YouTube', 'WhatsApp', 'Games'],\n",
        "        'Hours': [3.5, 2.8, 4.2, 1.5, 2.1],\n",
        "        'Pickups': [45, 38, 25, 60, 15]\n",
        "    })\n",
        "\n",
        "    # Analyze screen time\n",
        "    screen_analysis = analyze_screen_time_with_claude(sample_screen_time)\n",
        "    print(\"Screen Time Analysis:\", json.dumps(screen_analysis, indent=2))\n",
        "\n",
        "# Uncomment to run example\n",
        "# example_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O--WCHIeNDU",
        "outputId": "5b79d7c0-3641-41b8-86c4-73a64da60855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Please set your ANTHROPIC_API_KEY in your .env file\n",
            "You can get your API key from: https://console.anthropic.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVsfG5FrllaW",
        "outputId": "eefd0f35-02e2-4f49-ad91-fd35c6d7e0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.54.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
      ],
      "metadata": {
        "id": "xhFUH_t8oHpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import anthropic\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "claude_api_key = os.getenv(\"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\")\n",
        "\n",
        "# Claude client setup\n",
        "claude_client = anthropic.Anthropic(api_key=claude_api_key)\n",
        "\n",
        "# Function to analyze chat using Claude\n",
        "def analyze_chat_with_claude(messages, n=50):\n",
        "    \"\"\"Analyze WhatsApp messages using Claude AI\"\"\"\n",
        "    if not isinstance(messages, list) or not messages:\n",
        "        return {\"error\": \"Invalid messages input\"}\n",
        "\n",
        "    # Join the last n messages\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_template = f\"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages for a teenager's mental health:\n",
        "- Emotional tone (stress, joy, anxiety, neutral)\n",
        "- Mental clarity & decision style (clear, confused, impulsive)\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid for better mental health\n",
        "- 3 uplifting movies and 3 calming songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Chat messages:\n",
        "{recent}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract the JSON from Claude's response\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Try to find JSON in the response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            return json.loads(response_text)\n",
        "        else:\n",
        "            # Look for JSON within the response\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                return json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing chat with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze chat: {str(e)}\",\n",
        "            \"emotional_tone\": \"Unknown\",\n",
        "            \"clarity\": \"Unknown\",\n",
        "            \"mindset\": \"Unknown\",\n",
        "            \"avoid\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Screen Time Analysis with Claude\n",
        "def load_screen_time(csv_path):\n",
        "    \"\"\"Load screen time data from CSV\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"‚úÖ Loaded screen time data: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time_with_claude(df):\n",
        "    \"\"\"Analyze screen time data using Claude AI\"\"\"\n",
        "    if df.empty:\n",
        "        return {\"error\": \"No screen time data provided\"}\n",
        "\n",
        "    # Convert dataframe to readable format (limit rows for API)\n",
        "    sample_data = df.head(20).to_string(index=False)\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a digital wellness AI expert.\n",
        "\n",
        "Analyze this screen time data for a teenager:\n",
        "- Assess focus vs distraction patterns\n",
        "- Identify signs of digital burnout, overuse, or addiction\n",
        "- Look for decision fatigue indicators\n",
        "\n",
        "Provide recommendations:\n",
        "- Mental clarity score (0-100, where 100 is excellent focus)\n",
        "- Apps to avoid or limit\n",
        "- 3 inspiring movies and 3 calming songs for digital detox\n",
        "- 3 practical digital detox habits\n",
        "\n",
        "Output ONLY in valid JSON format as:\n",
        "{{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...]}}\n",
        "\n",
        "Screen Time Data:\n",
        "{sample_data}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.3,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        response_text = response.content[0].text.strip()\n",
        "\n",
        "        # Extract JSON from response\n",
        "        if response_text.startswith('{') and response_text.endswith('}'):\n",
        "            data = json.loads(response_text)\n",
        "        else:\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                data = json.loads(json_str)\n",
        "            else:\n",
        "                raise ValueError(\"No JSON found in response\")\n",
        "\n",
        "        # Validate and clean clarity_score\n",
        "        try:\n",
        "            clarity_score = int(data.get(\"clarity_score\", 50))\n",
        "            data[\"clarity_score\"] = max(0, min(100, clarity_score))\n",
        "        except:\n",
        "            data[\"clarity_score\"] = 50\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error analyzing screen time with Claude: {e}\")\n",
        "        return {\n",
        "            \"error\": f\"Failed to analyze screen time: {str(e)}\",\n",
        "            \"clarity_score\": 50,\n",
        "            \"fatigue\": \"Unknown\",\n",
        "            \"avoid_apps\": [],\n",
        "            \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "            \"habits\": []\n",
        "        }\n",
        "\n",
        "# Twitter Sentiment Analysis with Claude\n",
        "def analyze_tweet_sentiment_claude(tweet):\n",
        "    \"\"\"Analyze sentiment of a single tweet using Claude\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Analyze the sentiment of this tweet. Respond with exactly one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Tweet: \"{tweet}\"\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",  # Using Haiku for faster sentiment analysis\n",
        "            max_tokens=10,\n",
        "            temperature=0,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        sentiment = response.content[0].text.strip()\n",
        "\n",
        "        # Validate sentiment\n",
        "        valid_sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
        "        if sentiment.lower() in valid_sentiments:\n",
        "            return sentiment.capitalize()\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error analyzing tweet sentiment: {e}\")\n",
        "        return \"Neutral\"\n",
        "\n",
        "def analyze_tweets_with_claude(df):\n",
        "    \"\"\"Analyze sentiment for all tweets in dataframe\"\"\"\n",
        "    print(\"üîç Columns in CSV:\", df.columns.tolist())\n",
        "\n",
        "    # Find tweet column\n",
        "    tweet_col = None\n",
        "    for col in df.columns:\n",
        "        if col.strip().lower() in [\"tweet\", \"text\", \"message\", \"content\"]:\n",
        "            tweet_col = col\n",
        "            break\n",
        "\n",
        "    if not tweet_col:\n",
        "        string_cols = df.select_dtypes(include='object')\n",
        "        if not string_cols.empty:\n",
        "            tweet_col = string_cols.apply(lambda c: c.str.len().mean()).idxmax()\n",
        "            print(f\"‚úÖ Auto-selected tweet column: '{tweet_col}'\")\n",
        "        else:\n",
        "            print(\"‚ùå No suitable text column found\")\n",
        "            return df\n",
        "\n",
        "    # Analyze sentiments with progress bar\n",
        "    tqdm.pandas(desc=\"Analyzing tweet sentiments\")\n",
        "    df[\"sentiment\"] = df[tweet_col].progress_apply(analyze_tweet_sentiment_claude)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Final Report Synthesis with Claude\n",
        "def synthesize_report_with_claude(chat_json, screen_json, sentiment_df):\n",
        "    \"\"\"Generate final mental health report using Claude\"\"\"\n",
        "\n",
        "    # Get sentiment summary\n",
        "    if \"sentiment\" in sentiment_df.columns:\n",
        "        sentiment_counts = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "        sentiment_summary = f\"Sentiment distribution: {sentiment_counts}\"\n",
        "    else:\n",
        "        sentiment_summary = \"No sentiment data available\"\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "You are a compassionate NeuroAI mental health advisor from the future.\n",
        "\n",
        "Synthesize these data sources to create a comprehensive teen mental health summary:\n",
        "\n",
        "1. WhatsApp Chat Analysis:\n",
        "{json.dumps(chat_json, indent=2) if isinstance(chat_json, dict) else str(chat_json)}\n",
        "\n",
        "2. Screen Time Analysis:\n",
        "{json.dumps(screen_json, indent=2) if isinstance(screen_json, dict) else str(screen_json)}\n",
        "\n",
        "3. Social Media Sentiment Analysis:\n",
        "{sentiment_summary}\n",
        "\n",
        "Please provide a warm, supportive, and actionable summary that includes:\n",
        "- Overall mood and stress patterns\n",
        "- Top 3 mental health concerns identified\n",
        "- Curated list of mindfulness movies and calming songs\n",
        "- 5 futuristic but practical daily habits for better mental wellness\n",
        "\n",
        "Write in a caring, encouraging tone as if speaking directly to the teenager.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-sonnet-20240229\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.4,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt_template}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return response.content[0].text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating final report: {e}\")\n",
        "        return f\"Unable to generate comprehensive report due to error: {str(e)}\"\n",
        "\n",
        "# Mock function for WhatsApp extraction (replace with actual implementation)\n",
        "def extract_whatsapp_messages(file_path):\n",
        "    \"\"\"Mock function - replace with actual WhatsApp extraction logic\"\"\"\n",
        "    return [\n",
        "        \"User1: Feeling so stressed about exams üò∞\",\n",
        "        \"User2: Same here, can't sleep properly\",\n",
        "        \"User1: Social media is making me feel worse\",\n",
        "        \"User2: Maybe we should take a break from it\",\n",
        "        \"User1: Yeah, let's try some offline activities\",\n",
        "        \"User2: Good idea! Want to go for a walk?\"\n",
        "    ]\n",
        "\n",
        "# MAIN PIPELINE\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"üß† Starting Mental Health Analysis with Claude AI...\")\n",
        "\n",
        "    # File paths (update these to your actual file paths)\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        # 1. WhatsApp Analysis\n",
        "        print(\"\\nüì± Analyzing WhatsApp messages...\")\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat_with_claude(chat_msgs) if chat_msgs else {\"error\": \"No messages found\"}\n",
        "        print(\"‚úÖ WhatsApp analysis complete\")\n",
        "\n",
        "        # 2. Screen Time Analysis\n",
        "        print(\"\\n‚è∞ Analyzing screen time data...\")\n",
        "        df_screen = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time_with_claude(df_screen) if not df_screen.empty else {\"error\": \"No screen time data\"}\n",
        "        print(\"‚úÖ Screen time analysis complete\")\n",
        "\n",
        "        # 3. Twitter Sentiment Analysis\n",
        "        print(\"\\nüê¶ Analyzing social media sentiment...\")\n",
        "        try:\n",
        "            df_tweets = pd.read_csv(twitter_file)\n",
        "        except:\n",
        "            # Create sample data if file doesn't exist\n",
        "            df_tweets = pd.DataFrame({\n",
        "                \"tweet\": [\n",
        "                    \"Having such a great day! üòä\",\n",
        "                    \"Feeling overwhelmed with everything...\",\n",
        "                    \"Just finished a good workout, feeling energized!\",\n",
        "                    \"Can't focus on anything today üòî\",\n",
        "                    \"Grateful for my friends and family ‚ù§Ô∏è\"\n",
        "                ]\n",
        "            })\n",
        "\n",
        "        sentiment_df = analyze_tweets_with_claude(df_tweets)\n",
        "        print(\"‚úÖ Sentiment analysis complete\")\n",
        "\n",
        "        # 4. Generate Final Report\n",
        "        print(\"\\nüìã Generating comprehensive mental health report...\")\n",
        "        final_report = synthesize_report_with_claude(chat_result, screen_result, sentiment_df)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üß† COMPREHENSIVE MENTAL HEALTH ANALYSIS REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        print(final_report)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Save results to JSON file\n",
        "        results = {\n",
        "            \"chat_analysis\": chat_result,\n",
        "            \"screen_time_analysis\": screen_result,\n",
        "            \"sentiment_analysis\": sentiment_df.to_dict('records') if not sentiment_df.empty else [],\n",
        "            \"final_report\": final_report,\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "\n",
        "        with open(\"mental_health_analysis_results.json\", \"w\") as f:\n",
        "            json.dump(results, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"\\nüíæ Results saved to 'mental_health_analysis_results.json'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline Error: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if API key is available\n",
        "    if not claude_api_key:\n",
        "        print(\"‚ùå Please set your ANTHROPIC_API_KEY in your .env file\")\n",
        "        print(\"You can get your API key from: https://console.anthropic.com/\")\n",
        "    else:\n",
        "        main()\n",
        "\n",
        "# Example of how to use individual functions\n",
        "def example_usage():\n",
        "    \"\"\"Example of how to use the individual analysis functions\"\"\"\n",
        "\n",
        "    # Example chat messages\n",
        "    sample_messages = [\n",
        "        \"Feeling really anxious about school lately\",\n",
        "        \"Social media is making me feel worse about myself\",\n",
        "        \"Maybe I should take a break from Instagram\",\n",
        "        \"My sleep schedule is completely messed up\",\n",
        "        \"I want to feel better but don't know how\"\n",
        "    ]\n",
        "\n",
        "    # Analyze chat\n",
        "    chat_analysis = analyze_chat_with_claude(sample_messages)\n",
        "    print(\"Chat Analysis:\", json.dumps(chat_analysis, indent=2))\n",
        "\n",
        "    # Create sample screen time data\n",
        "    sample_screen_time = pd.DataFrame({\n",
        "        'App': ['Instagram', 'TikTok', 'YouTube', 'WhatsApp', 'Games'],\n",
        "        'Hours': [3.5, 2.8, 4.2, 1.5, 2.1],\n",
        "        'Pickups': [45, 38, 25, 60, 15]\n",
        "    })\n",
        "\n",
        "    # Analyze screen time\n",
        "    screen_analysis = analyze_screen_time_with_claude(sample_screen_time)\n",
        "    print(\"Screen Time Analysis:\", json.dumps(screen_analysis, indent=2))\n",
        "\n",
        "# Uncomment to run example\n",
        "# example_usage()\n",
        "\n",
        "\n",
        "# --- INPUT JSON ---\n",
        "data = '''\n",
        "{\n",
        "  \"mood\": \"Stressed and anxious with periods of joy\",\n",
        "  \"top_issues\": [\"Overuse of social media\", \"Sleep deprivation\", \"Lack of mental clarity\"],\n",
        "  \"recommended_movies\": [\"Inside Out\", \"The Pursuit of Happyness\", \"Soul\"],\n",
        "  \"recommended_songs\": [\"Weightless - Marconi Union\", \"Lovely Day - Bill Withers\", \"Here Comes the Sun - The Beatles\"],\n",
        "  \"habits\": [\"Daily journaling\", \"30-minute screen-free walk\", \"Night-time digital detox routine\"]\n",
        "}\n",
        "'''\n",
        "\n",
        "# --- LOAD JSON INTO PYTHON DICTIONARY ---\n",
        "analysis = json.loads(data)\n",
        "\n",
        "# --- PRINT IN A CLEAN FORMAT ---\n",
        "print(\"\\nüß† Mood Summary:\")\n",
        "print(f\"  - Mood: {analysis['mood']}\")\n",
        "\n",
        "print(\"\\nüö© Top Issues Detected:\")\n",
        "for issue in analysis['top_issues']:\n",
        "    print(f\"  - {issue}\")\n",
        "\n",
        "print(\"\\nüé¨ Recommended Movies:\")\n",
        "for movie in analysis['recommended_movies']:\n",
        "    print(f\"  - {movie}\")\n",
        "\n",
        "print(\"\\nüéµ Recommended Songs:\")\n",
        "for song in analysis['recommended_songs']:\n",
        "    print(f\"  - {song}\")\n",
        "\n",
        "print(\"\\nüåø Suggested Mental Health Habits:\")\n",
        "for habit in analysis['habits']:\n",
        "    print(f\"  - {habit}\")\n",
        "\n",
        "print(f\"üß† Mood: {analysis['mood']} | üö© Issues: {', '.join(analysis['top_issues'])} | üé¨ Movies: {', '.join(analysis['recommended_movies'])} | üéµ Songs: {', '.join(analysis['recommended_songs'])} | üåø Habits: {', '.join(analysis['habits'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYVqrBoSkiKL",
        "outputId": "cdb21341-796e-4a95-e17e-186829d127a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Please set your ANTHROPIC_API_KEY in your .env file\n",
            "You can get your API key from: https://console.anthropic.com/\n",
            "\n",
            "üß† Mood Summary:\n",
            "  - Mood: Stressed and anxious with periods of joy\n",
            "\n",
            "üö© Top Issues Detected:\n",
            "  - Overuse of social media\n",
            "  - Sleep deprivation\n",
            "  - Lack of mental clarity\n",
            "\n",
            "üé¨ Recommended Movies:\n",
            "  - Inside Out\n",
            "  - The Pursuit of Happyness\n",
            "  - Soul\n",
            "\n",
            "üéµ Recommended Songs:\n",
            "  - Weightless - Marconi Union\n",
            "  - Lovely Day - Bill Withers\n",
            "  - Here Comes the Sun - The Beatles\n",
            "\n",
            "üåø Suggested Mental Health Habits:\n",
            "  - Daily journaling\n",
            "  - 30-minute screen-free walk\n",
            "  - Night-time digital detox routine\n",
            "üß† Mood: Stressed and anxious with periods of joy | üö© Issues: Overuse of social media, Sleep deprivation, Lack of mental clarity | üé¨ Movies: Inside Out, The Pursuit of Happyness, Soul | üéµ Songs: Weightless - Marconi Union, Lovely Day - Bill Withers, Here Comes the Sun - The Beatles | üåø Habits: Daily journaling, 30-minute screen-free walk, Night-time digital detox routine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_bxKETL9Him"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import zipfile\n",
        "from blackbox_ai import BlackboxAI  # Hypothetical import for Blackbox AI\n",
        "\n",
        "# Load environment variables and API key\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"BLACKBOX_API_KEY\")  # Assuming Blackbox AI uses a different key\n",
        "\n",
        "# Initialize Blackbox AI model\n",
        "bb_ai = BlackboxAI(api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                chat_data = f.read().decode('utf-8')\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(chat=recent)\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        return response\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(data=readable)\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json):\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2),\n",
        "        screen_json=json.dumps(screen_json, indent=2)\n",
        "    )\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    return response\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "\n",
        "    chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "    chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "    df_screen = load_screen_time(screen_time_file)\n",
        "    screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "    final_report = synthesize_report(chat_result, screen_result)\n",
        "    print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "LBoZDI0Y9G_m",
        "outputId": "8b7e02dd-7e42-4c3d-a003-ea445ed39d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'blackbox_ai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4174165755>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mblackbox_ai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlackboxAI\u001b[0m  \u001b[0;31m# Hypothetical import for Blackbox AI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load environment variables and API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blackbox_ai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai python-dotenv pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7XhbJY8_Z-P",
        "outputId": "0931e079-6868-4776-aecf-c7635a06dab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.88.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "63ilZxIa_sU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install BlackboxAI\n",
        "!import BlackboxAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f1jR-MU_xkP",
        "outputId": "c452a643-e454-4ced-823d-59a687053611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting BlackboxAI\n",
            "  Downloading blackboxai-3.3-py3-none-any.whl.metadata (750 bytes)\n",
            "Downloading blackboxai-3.3-py3-none-any.whl (275 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.6/275.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.5/275.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: BlackboxAI\n",
            "Successfully installed BlackboxAI-3.3\n",
            "/bin/bash: line 1: import: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import zipfile\n",
        "from blackbox_ai import BlackboxAI  # Hypothetical import for Blackbox AI\n",
        "\n",
        "# Load environment variables and API key\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"BLACKBOX_API_KEY\")  # Assuming Blackbox AI uses a different key\n",
        "\n",
        "# Initialize Blackbox AI model\n",
        "bb_ai = BlackboxAI(api_key=api_key)\n",
        "\n",
        "# --- WhatsApp Chat Extraction ---\n",
        "def extract_whatsapp_messages(zip_path):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "            if not txt_files:\n",
        "                print(\"No text files found in the ZIP.\")\n",
        "                return []\n",
        "            chat_file = txt_files[0]\n",
        "            with zip_ref.open(chat_file) as f:\n",
        "                chat_data = f.read().decode('utf-8')\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting chat:\", e)\n",
        "        return []\n",
        "\n",
        "    # Join multiline messages\n",
        "    lines = chat_data.split('\\n')\n",
        "    merged_lines = []\n",
        "    date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "    buffer = \"\"\n",
        "    for line in lines:\n",
        "        if date_pattern.match(line):\n",
        "            if buffer:\n",
        "                merged_lines.append(buffer)\n",
        "            buffer = line\n",
        "        else:\n",
        "            buffer += \" \" + line.strip()\n",
        "    if buffer:\n",
        "        merged_lines.append(buffer)\n",
        "\n",
        "    pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "    messages = []\n",
        "    for line in merged_lines:\n",
        "        match = pattern.match(line)\n",
        "        if match:\n",
        "            sender, msg = match.groups()\n",
        "            if msg.strip() and \"media omitted\" not in msg.lower():\n",
        "                messages.append(f\"{sender}: {msg}\")\n",
        "    return messages\n",
        "\n",
        "# --- WhatsApp Chat Analysis ---\n",
        "def analyze_chat(messages, n=50):\n",
        "    recent = \"\\n\".join(messages[-n:])\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(chat=recent)\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error decoding JSON response from BlackboxAI.\")\n",
        "        return response\n",
        "\n",
        "# --- Screen Time Analysis ---\n",
        "def load_screen_time(csv_path):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load screen time CSV: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def analyze_screen_time(df):\n",
        "    readable = df.to_string(index=False)\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Screen Time Data:\n",
        "{data}\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(data=readable)\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Error decoding JSON response from BlackboxAI.\")\n",
        "        return {\"clarity_score\": 50, \"fatigue\": \"Unknown\", \"avoid_apps\": [], \"recommend\": {\"movies\": [], \"songs\": []}, \"habits\": []}\n",
        "\n",
        "# --- Final Report Synthesis ---\n",
        "def synthesize_report(chat_json, screen_json):\n",
        "    prompt_template_str = \"\"\"\n",
        "You are a NeuroAI fusion advisor from the future.\n",
        "\n",
        "Combine these:\n",
        "1. WhatsApp analysis:\n",
        "{chat_json}\n",
        "\n",
        "2. Screen time report:\n",
        "{screen_json}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress pattern\n",
        "- Top 3 issues\n",
        "- Mindfulness movie/song list\n",
        "- Futuristic habit suggestions\n",
        "\n",
        "Respond warmly and clearly.\n",
        "\"\"\"\n",
        "    formatted_prompt = prompt_template_str.format(\n",
        "        chat_json=json.dumps(chat_json, indent=2),\n",
        "        screen_json=json.dumps(screen_json, indent=2)\n",
        "    )\n",
        "\n",
        "    response = bb_ai.analyze_text(formatted_prompt)  # Hypothetical method for analysis\n",
        "    return response\n",
        "\n",
        "# --- MAIN PIPELINE ---\n",
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "\n",
        "    chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "    chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No usable messages.\"\n",
        "\n",
        "    df_screen = load_screen_time(screen_time_file)\n",
        "    screen_result = analyze_screen_time(df_screen) if not df_screen.empty else \"No screen time data.\"\n",
        "\n",
        "    final_report = synthesize_report(chat_result, screen_result)\n",
        "    print(\"\\nüß† Final Mental Health Summary:\\n\", final_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tJCzGmiQ-fWx",
        "outputId": "8bb7a5e3-6ccd-41d9-935b-3d2be6d561a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BlackboxAI' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1785237812>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Initialize Blackbox AI model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbb_ai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlackboxAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- WhatsApp Chat Extraction ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BlackboxAI' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "grok_api_key = os.getenv(\"GROK_API_KEY\")\n",
        "\n",
        "# Grok API client\n",
        "class GrokClient:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://api.x.ai/grok\"  # Hypothetical endpoint; check https://x.ai/api\n",
        "\n",
        "    def generate(self, model, prompt, max_tokens):\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": model,  # e.g., \"grok-3\"\n",
        "            \"prompt\": prompt,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(self.base_url, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json().get(\"text\")  # Adjust based on actual API response structure\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå Grok API call failed: {e}\")\n",
        "            return json.dumps({\"error\": \"API failure\"})\n",
        "\n",
        "grok_client = GrokClient(api_key=grok_api_key)"
      ],
      "metadata": {
        "id": "fKqMZBYUY26P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "  {\n",
        "    \"input\": \"User1: I'm so stressed about exams!\\nUser2: Take a break, you got this!\",\n",
        "    \"output\": {\n",
        "      \"emotional_tone\": \"Stress\",\n",
        "      \"clarity\": \"Low\",\n",
        "      \"mindset\": \"Reactive\",\n",
        "      \"avoid\": [\"Social Media\", \"Late-night scrolling\", \"Gaming\"],\n",
        "      \"recommend\": {\"movies\": [\"Inside Out\", \"Soul\", \"The Pursuit of Happyness\"], \"songs\": [\"Weightless\", \"Lovely Day\", \"Here Comes the Sun\"]},\n",
        "      \"habits\": [\"Journaling\", \"Meditation\", \"Screen-free walk\"]\n",
        "    }\n",
        "  },\n",
        "  ...\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD-i0d-KZdRj",
        "outputId": "20d545d7-a8f4-47dd-88c9-fc0bcc9d6ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': \"User1: I'm so stressed about exams!\\nUser2: Take a break, you got this!\",\n",
              "  'output': {'emotional_tone': 'Stress',\n",
              "   'clarity': 'Low',\n",
              "   'mindset': 'Reactive',\n",
              "   'avoid': ['Social Media', 'Late-night scrolling', 'Gaming'],\n",
              "   'recommend': {'movies': ['Inside Out', 'Soul', 'The Pursuit of Happyness'],\n",
              "    'songs': ['Weightless', 'Lovely Day', 'Here Comes the Sun']},\n",
              "   'habits': ['Journaling', 'Meditation', 'Screen-free walk']}},\n",
              " Ellipsis]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\"prompt\": \"Analyze these WhatsApp messages: User1: I'm so stressed about exams!\\nUser2: Take a break, you got this!\", \"completion\": \"{\\\"emotional_tone\\\": \\\"Stress\\\", \\\"clarity\\\": \\\"Low\\\", \\\"mindset\\\": \\\"Reactive\\\", ...}\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xUc9YQ1Zhwp",
        "outputId": "e4fccbde-1ebc-4286-941e-c9bae861242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': \"Analyze these WhatsApp messages: User1: I'm so stressed about exams!\\nUser2: Take a break, you got this!\",\n",
              " 'completion': '{\"emotional_tone\": \"Stress\", \"clarity\": \"Low\", \"mindset\": \"Reactive\", ...}'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(\n",
        "    \"https://api.x.ai/fine-tune\",\n",
        "    headers={\"Authorization\": f\"Bearer {grok_api_key}\"},\n",
        "    json={\n",
        "        \"model\": \"grok-3\",\n",
        "        \"training_file\": \"data.jsonl\",\n",
        "        \"hyperparameters\": {\"epochs\": 3, \"learning_rate\": 0.0001}\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "S2q9KOG1ZlkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = grok_client.generate(\n",
        "    model=\"grok-3-finetuned-123\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=150\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8Z8BsBUEZqNf",
        "outputId": "85f90eb5-3ca4-4b80-b572-597d5e4b38df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prompt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-246377774.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m response = grok_client.generate(\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"grok-3-finetuned-123\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_str = \"\"\"\n",
        "You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format as:\n",
        "{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {\"movies\": [...], \"songs\": [...]}, \"habits\": [...] }\n",
        "\n",
        "Example:\n",
        "Input: \"User1: I'm so stressed about exams!\\nUser2: Take a break, you got this!\"\n",
        "Output: {\"emotional_tone\": \"Stress\", \"clarity\": \"Low\", \"mindset\": \"Reactive\", \"avoid\": [\"Social Media\", \"Gaming\", \"Late-night scrolling\"], \"recommend\": {\"movies\": [\"Inside Out\", \"Soul\", \"The Pursuit of Happyness\"], \"songs\": [\"Weightless\", \"Lovely Day\", \"Here Comes the Sun\"]}, \"habits\": [\"Journaling\", \"Meditation\", \"Screen-free walk\"]}\n",
        "\n",
        "Chat:\n",
        "{chat}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MogEbpVjZyuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNMY8CP1VBZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuJ2cy4BVBB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import aiohttp\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPTool:\n",
        "    \"\"\"Represents an MCP tool that can be called\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: Dict[str, Any]\n",
        "\n",
        "class MCPLLMConnector:\n",
        "    \"\"\"Connects MCP server with LLM model\"\"\"\n",
        "\n",
        "    def __init__(self, server_command: List[str], llm_api_url: str, llm_api_key: str):\n",
        "        self.server_command = server_command\n",
        "        self.llm_api_url = llm_api_url\n",
        "        self.llm_api_key = llm_api_key\n",
        "        self.session: Optional[ClientSession] = None\n",
        "        self.available_tools: List[MCPTool] = []\n",
        "\n",
        "    async def connect_mcp_server(self):\n",
        "        \"\"\"Connect to MCP server and get available tools\"\"\"\n",
        "        try:\n",
        "            # Create server parameters\n",
        "            server_params = StdioServerParameters(\n",
        "                command=self.server_command[0],\n",
        "                args=self.server_command[1:] if len(self.server_command) > 1 else []\n",
        "            )\n",
        "\n",
        "            # Connect to MCP server\n",
        "            async with stdio_client(server_params) as (read, write):\n",
        "                async with ClientSession(read, write) as session:\n",
        "                    self.session = session\n",
        "\n",
        "                    # Initialize the connection\n",
        "                    await session.initialize()\n",
        "\n",
        "                    # Get available tools\n",
        "                    result = await session.list_tools()\n",
        "\n",
        "                    # Parse tools\n",
        "                    self.available_tools = []\n",
        "                    for tool in result.tools:\n",
        "                        mcp_tool = MCPTool(\n",
        "                            name=tool.name,\n",
        "                            description=tool.description or \"\",\n",
        "                            input_schema=tool.inputSchema or {}\n",
        "                        )\n",
        "                        self.available_tools.append(mcp_tool)\n",
        "                        logger.info(f\"Loaded MCP tool: {tool.name}\")\n",
        "\n",
        "                    return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to connect to MCP server: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call an MCP tool with given arguments\"\"\"\n",
        "        if not self.session:\n",
        "            raise Exception(\"MCP session not initialized\")\n",
        "\n",
        "        try:\n",
        "            result = await self.session.call_tool(tool_name, arguments)\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"result\": result.content,\n",
        "                \"tool_name\": tool_name\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling MCP tool {tool_name}: {e}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"tool_name\": tool_name\n",
        "            }\n",
        "\n",
        "    def format_tools_for_llm(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Format MCP tools for LLM function calling\"\"\"\n",
        "        formatted_tools = []\n",
        "\n",
        "        for tool in self.available_tools:\n",
        "            formatted_tool = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.input_schema\n",
        "                }\n",
        "            }\n",
        "            formatted_tools.append(formatted_tool)\n",
        "\n",
        "        return formatted_tools\n",
        "\n",
        "    async def call_llm_with_tools(self, messages: List[Dict[str, str]], model: str = \"gpt-4\") -> Dict[str, Any]:\n",
        "        \"\"\"Call LLM with available MCP tools\"\"\"\n",
        "\n",
        "        # Prepare the request payload\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": self.format_tools_for_llm(),\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.llm_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(self.llm_api_url, json=payload, headers=headers) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.json()\n",
        "                else:\n",
        "                    error_text = await response.text()\n",
        "                    raise Exception(f\"LLM API error: {response.status} - {error_text}\")\n",
        "\n",
        "    async def process_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from LLM response\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call[\"function\"][\"name\"]\n",
        "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "\n",
        "            # Call the MCP tool\n",
        "            result = await self.call_mcp_tool(function_name, function_args)\n",
        "\n",
        "            # Format result for LLM\n",
        "            tool_result = {\n",
        "                \"tool_call_id\": tool_call[\"id\"],\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": json.dumps(result)\n",
        "            }\n",
        "            results.append(tool_result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def chat_with_mcp_tools(self, user_message: str, model: str = \"gpt-4\") -> str:\n",
        "        \"\"\"Complete chat interaction using MCP tools\"\"\"\n",
        "\n",
        "        # Initialize conversation\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to various tools. Use them when needed to answer user questions.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        # Call LLM\n",
        "        llm_response = await self.call_llm_with_tools(messages, model)\n",
        "\n",
        "        # Check if LLM wants to use tools\n",
        "        choice = llm_response[\"choices\"][0]\n",
        "        message = choice[\"message\"]\n",
        "\n",
        "        if message.get(\"tool_calls\"):\n",
        "            # Process tool calls\n",
        "            tool_results = await self.process_tool_calls(message[\"tool_calls\"])\n",
        "\n",
        "            # Add assistant message and tool results to conversation\n",
        "            messages.append(message)\n",
        "            messages.extend(tool_results)\n",
        "\n",
        "            # Call LLM again with tool results\n",
        "            final_response = await self.call_llm_with_tools(messages, model)\n",
        "            return final_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            return message[\"content\"]\n",
        "\n",
        "# Example usage\n",
        "async def main():\n",
        "    # Configuration\n",
        "    MCP_SERVER_COMMAND = [\"python\", \"-m\", \"your_mcp_server\"]  # Replace with your MCP server command\n",
        "    LLM_API_URL = \"https://api.openai.com/v1/chat/completions\"  # Or your LLM endpoint\n",
        "    LLM_API_KEY = \"your-api-key-here\"  # Replace with your API key\n",
        "\n",
        "    # Create connector\n",
        "    connector = MCPLLMConnector(MCP_SERVER_COMMAND, LLM_API_URL, LLM_API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Connect to MCP server\n",
        "        logger.info(\"Connecting to MCP server...\")\n",
        "        success = await connector.connect_mcp_server()\n",
        "\n",
        "        if not success:\n",
        "            logger.error(\"Failed to connect to MCP server\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Connected! Available tools: {[tool.name for tool in connector.available_tools]}\")\n",
        "\n",
        "        # Example chat\n",
        "        user_query = \"What's the weather like today?\"\n",
        "        response = await connector.chat_with_mcp_tools(user_query)\n",
        "\n",
        "        print(f\"User: {user_query}\")\n",
        "        print(f\"Assistant: {response}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "\n",
        "# Alternative: Simple MCP Tool Wrapper\n",
        "class SimpleMCPWrapper:\n",
        "    \"\"\"Simplified wrapper for basic MCP integration\"\"\"\n",
        "\n",
        "    def __init__(self, mcp_tools: List[MCPTool]):\n",
        "        self.tools = mcp_tools\n",
        "\n",
        "    def create_system_prompt(self) -> str:\n",
        "        \"\"\"Create system prompt with tool descriptions\"\"\"\n",
        "        tool_descriptions = []\n",
        "        for tool in self.tools:\n",
        "            desc = f\"- {tool.name}: {tool.description}\"\n",
        "            tool_descriptions.append(desc)\n",
        "\n",
        "        return f\"\"\"You are an AI assistant with access to the following tools:\n",
        "{chr(10).join(tool_descriptions)}\n",
        "\n",
        "When you need to use a tool, respond with a JSON object in this format:\n",
        "{{\"action\": \"use_tool\", \"tool\": \"tool_name\", \"arguments\": {{\"param\": \"value\"}}}}\n",
        "\n",
        "Otherwise, respond normally.\"\"\"\n",
        "\n",
        "    def parse_llm_response(self, response: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Parse LLM response for tool usage\"\"\"\n",
        "        try:\n",
        "            # Try to parse as JSON\n",
        "            parsed = json.loads(response.strip())\n",
        "            if parsed.get(\"action\") == \"use_tool\":\n",
        "                return parsed\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "# Example MCP server setup (pseudo-code)\n",
        "\"\"\"\n",
        "# your_mcp_server.py\n",
        "from mcp.server import Server\n",
        "from mcp.server.stdio import stdio_server\n",
        "from mcp.types import Tool\n",
        "\n",
        "app = Server(\"example-server\")\n",
        "\n",
        "@app.list_tools()\n",
        "async def list_tools():\n",
        "    return [\n",
        "        Tool(\n",
        "            name=\"get_weather\",\n",
        "            description=\"Get current weather for a location\",\n",
        "            inputSchema={\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        )\n",
        "    ]\n",
        "\n",
        "@app.call_tool()\n",
        "async def call_tool(name: str, arguments: dict):\n",
        "    if name == \"get_weather\":\n",
        "        location = arguments[\"location\"]\n",
        "        # Your weather API logic here\n",
        "        return f\"The weather in {location} is sunny, 72¬∞F\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(stdio_server(app))\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Oe4BnBBoswo1",
        "outputId": "6b36fd61-8c4a-4766-c6b8-c2512c0c2b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mcp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1154927701.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmcp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStdioServerParameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstdio_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mcp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the MCP package:\n",
        "# !pip install mcp\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import subprocess\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import aiohttp\n",
        "\n",
        "# Alternative implementation without MCP imports (fallback)\n",
        "# Uncomment the following when MCP is properly installed:\n",
        "# from mcp import ClientSession, StdioServerParameters\n",
        "# from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPTool:\n",
        "    \"\"\"Represents an MCP tool that can be called\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: Dict[str, Any]\n",
        "\n",
        "class MCPLLMConnector:\n",
        "    \"\"\"Connects MCP server with LLM model - Alternative implementation\"\"\"\n",
        "\n",
        "    def __init__(self, server_command: List[str], llm_api_url: str, llm_api_key: str):\n",
        "        self.server_command = server_command\n",
        "        self.llm_api_url = llm_api_url\n",
        "        self.llm_api_key = llm_api_key\n",
        "        self.process: Optional[subprocess.Popen] = None\n",
        "        self.available_tools: List[MCPTool] = []\n",
        "\n",
        "    async def connect_mcp_server(self):\n",
        "        \"\"\"Connect to MCP server using subprocess (fallback method)\"\"\"\n",
        "        try:\n",
        "            # Start MCP server process\n",
        "            self.process = subprocess.Popen(\n",
        "                self.server_command,\n",
        "                stdin=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            # Send initialization message\n",
        "            init_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 1,\n",
        "                \"method\": \"initialize\",\n",
        "                \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}\n",
        "            }\n",
        "\n",
        "            await self._send_message(init_message)\n",
        "\n",
        "            # Get available tools\n",
        "            tools_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 2,\n",
        "                \"method\": \"tools/list\",\n",
        "                \"params\": {}\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tools_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                tools = response[\"result\"].get(\"tools\", [])\n",
        "                self.available_tools = []\n",
        "\n",
        "                for tool in tools:\n",
        "                    mcp_tool = MCPTool(\n",
        "                        name=tool.get(\"name\", \"\"),\n",
        "                        description=tool.get(\"description\", \"\"),\n",
        "                        input_schema=tool.get(\"inputSchema\", {})\n",
        "                    )\n",
        "                    self.available_tools.append(mcp_tool)\n",
        "                    logger.info(f\"Loaded MCP tool: {tool.get('name')}\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to connect to MCP server: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def _send_message(self, message: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Send JSON-RPC message to MCP server\"\"\"\n",
        "        if not self.process:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Send message\n",
        "            message_str = json.dumps(message) + \"\\n\"\n",
        "            self.process.stdin.write(message_str)\n",
        "            self.process.stdin.flush()\n",
        "\n",
        "            # Read response\n",
        "            response_str = self.process.stdout.readline()\n",
        "            if response_str:\n",
        "                return json.loads(response_str.strip())\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error communicating with MCP server: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call an MCP tool with given arguments\"\"\"\n",
        "        if not self.process:\n",
        "            raise Exception(\"MCP server not initialized\")\n",
        "\n",
        "        try:\n",
        "            # Send tool call message\n",
        "            tool_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 3,\n",
        "                \"method\": \"tools/call\",\n",
        "                \"params\": {\n",
        "                    \"name\": tool_name,\n",
        "                    \"arguments\": arguments\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tool_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"result\": response[\"result\"],\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"error\": \"No result from MCP server\",\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling MCP tool {tool_name}: {e}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"tool_name\": tool_name\n",
        "            }\n",
        "\n",
        "    def format_tools_for_llm(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Format MCP tools for LLM function calling\"\"\"\n",
        "        formatted_tools = []\n",
        "\n",
        "        for tool in self.available_tools:\n",
        "            formatted_tool = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.input_schema\n",
        "                }\n",
        "            }\n",
        "            formatted_tools.append(formatted_tool)\n",
        "\n",
        "        return formatted_tools\n",
        "\n",
        "    async def call_llm_with_tools(self, messages: List[Dict[str, str]], model: str = \"gpt-4\") -> Dict[str, Any]:\n",
        "        \"\"\"Call LLM with available MCP tools\"\"\"\n",
        "\n",
        "        # Prepare the request payload\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": self.format_tools_for_llm(),\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.llm_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(self.llm_api_url, json=payload, headers=headers) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.json()\n",
        "                else:\n",
        "                    error_text = await response.text()\n",
        "                    raise Exception(f\"LLM API error: {response.status} - {error_text}\")\n",
        "\n",
        "    async def process_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from LLM response\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call[\"function\"][\"name\"]\n",
        "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "\n",
        "            # Call the MCP tool\n",
        "            result = await self.call_mcp_tool(function_name, function_args)\n",
        "\n",
        "            # Format result for LLM\n",
        "            tool_result = {\n",
        "                \"tool_call_id\": tool_call[\"id\"],\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": json.dumps(result)\n",
        "            }\n",
        "            results.append(tool_result)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up MCP server process\"\"\"\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            self.process.wait()\n",
        "            self.process = None\n",
        "\n",
        "    async def chat_with_mcp_tools(self, user_message: str, model: str = \"gpt-4\") -> str:\n",
        "        \"\"\"Complete chat interaction using MCP tools\"\"\"\n",
        "\n",
        "        # Initialize conversation\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to various tools. Use them when needed to answer user questions.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        # Call LLM\n",
        "        llm_response = await self.call_llm_with_tools(messages, model)\n",
        "\n",
        "        # Check if LLM wants to use tools\n",
        "        choice = llm_response[\"choices\"][0]\n",
        "        message = choice[\"message\"]\n",
        "\n",
        "        if message.get(\"tool_calls\"):\n",
        "            # Process tool calls\n",
        "            tool_results = await self.process_tool_calls(message[\"tool_calls\"])\n",
        "\n",
        "            # Add assistant message and tool results to conversation\n",
        "            messages.append(message)\n",
        "            messages.extend(tool_results)\n",
        "\n",
        "            # Call LLM again with tool results\n",
        "            final_response = await self.call_llm_with_tools(messages, model)\n",
        "            return final_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            return message[\"content\"]\n",
        "\n",
        "# Installation and usage example\n",
        "async def main():\n",
        "    # First install required packages\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"aiohttp\"], check=True)\n",
        "\n",
        "    # For proper MCP support, also install:\n",
        "    # subprocess.run([\"pip\", \"install\", \"mcp\"], check=True)\n",
        "\n",
        "    # Configuration\n",
        "    MCP_SERVER_COMMAND = [\"python\", \"-m\", \"simple_mcp_server\"]  # Your MCP server\n",
        "    LLM_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
        "    LLM_API_KEY = \"your-api-key-here\"  # Replace with actual key\n",
        "\n",
        "    # Create connector\n",
        "    connector = MCPLLMConnector(MCP_SERVER_COMMAND, LLM_API_URL, LLM_API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Connect to MCP server\n",
        "        logger.info(\"Connecting to MCP server...\")\n",
        "        success = await connector.connect_mcp_server()\n",
        "\n",
        "        if not success:\n",
        "            logger.error(\"Failed to connect to MCP server\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Connected! Available tools: {[tool.name for tool in connector.available_tools]}\")\n",
        "\n",
        "        # Example chat\n",
        "        user_query = \"What tools are available?\"\n",
        "        if connector.available_tools:\n",
        "            tool_list = \", \".join([tool.name for tool in connector.available_tools])\n",
        "            print(f\"Available MCP tools: {tool_list}\")\n",
        "        else:\n",
        "            print(\"No MCP tools available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "    finally:\n",
        "        # Clean up\n",
        "        connector.cleanup()\n",
        "\n",
        "# Simple standalone MCP-style tool manager (no external dependencies)\n",
        "class SimpleMCPManager:\n",
        "    \"\"\"Simple tool manager that mimics MCP functionality without external deps\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tools = {}\n",
        "        self.setup_default_tools()\n",
        "\n",
        "    def setup_default_tools(self):\n",
        "        \"\"\"Setup some default tools for demonstration\"\"\"\n",
        "        self.tools = {\n",
        "            \"get_time\": {\n",
        "                \"name\": \"get_time\",\n",
        "                \"description\": \"Get current time\",\n",
        "                \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
        "            },\n",
        "            \"calculate\": {\n",
        "                \"name\": \"calculate\",\n",
        "                \"description\": \"Perform basic calculations\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
        "                    },\n",
        "                    \"required\": [\"expression\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def list_tools(self):\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        return list(self.tools.values())\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call a tool with arguments\"\"\"\n",
        "        if tool_name == \"get_time\":\n",
        "            import datetime\n",
        "            return {\"result\": f\"Current time: {datetime.datetime.now()}\"}\n",
        "        elif tool_name == \"calculate\":\n",
        "            try:\n",
        "                expression = arguments.get(\"expression\", \"\")\n",
        "                # Simple safe evaluation (be careful in production!)\n",
        "                result = eval(expression.replace(\"^\", \"**\"))\n",
        "                return {\"result\": f\"{expression} = {result}\"}\n",
        "            except Exception as e:\n",
        "                return {\"error\": f\"Calculation error: {str(e)}\"}\n",
        "        else:\n",
        "            return {\"error\": f\"Unknown tool: {tool_name}\"}\n",
        "\n",
        "# Alternative main function using SimpleMCPManager\n",
        "async def simple_main():\n",
        "    \"\"\"Simple example without external MCP dependencies\"\"\"\n",
        "    manager = SimpleMCPManager()\n",
        "\n",
        "    print(\"Available tools:\")\n",
        "    for tool in manager.list_tools():\n",
        "        print(f\"- {tool['name']}: {tool['description']}\")\n",
        "\n",
        "    # Test tool calls\n",
        "    print(\"\\nTesting tools:\")\n",
        "\n",
        "    time_result = await manager.call_tool(\"get_time\", {})\n",
        "    print(f\"Time tool: {time_result}\")\n",
        "\n",
        "    calc_result = await manager.call_tool(\"calculate\", {\"expression\": \"2 + 2 * 3\"})\n",
        "    print(f\"Calculator tool: {calc_result}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Choose execution mode:\")\n",
        "    print(\"1. Full MCP integration (requires MCP package)\")\n",
        "    print(\"2. Simple demo without external dependencies\")\n",
        "\n",
        "    choice = input(\"Enter choice (1 or 2): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        asyncio.run(main())\n",
        "    else:\n",
        "        asyncio.run(simple_main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "OTWFZGgIt16K",
        "outputId": "0ebf3f5d-13f4-4b17-8985-f07cef8fef96"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose execution mode:\n",
            "1. Full MCP integration (requires MCP package)\n",
            "2. Simple demo without external dependencies\n",
            "Enter choice (1 or 2): 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1803542026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the MCP package:\n",
        "# !pip install mcp\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import subprocess\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import aiohttp\n",
        "\n",
        "# Alternative implementation without MCP imports (fallback)\n",
        "# Uncomment the following when MCP is properly installed:\n",
        "# from mcp import ClientSession, StdioServerParameters\n",
        "# from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPTool:\n",
        "    \"\"\"Represents an MCP tool that can be called\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: Dict[str, Any]\n",
        "\n",
        "class MCPLLMConnector:\n",
        "    \"\"\"Connects MCP server with LLM model - Alternative implementation\"\"\"\n",
        "\n",
        "    def __init__(self, server_command: List[str], llm_api_url: str, llm_api_key: str):\n",
        "        self.server_command = server_command\n",
        "        self.llm_api_url = llm_api_url\n",
        "        self.llm_api_key = llm_api_key\n",
        "        self.process: Optional[subprocess.Popen] = None\n",
        "        self.available_tools: List[MCPTool] = []\n",
        "\n",
        "    async def connect_mcp_server(self):\n",
        "        \"\"\"Connect to MCP server using subprocess (fallback method)\"\"\"\n",
        "        try:\n",
        "            # Start MCP server process\n",
        "            self.process = subprocess.Popen(\n",
        "                self.server_command,\n",
        "                stdin=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            # Send initialization message\n",
        "            init_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 1,\n",
        "                \"method\": \"initialize\",\n",
        "                \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}\n",
        "            }\n",
        "\n",
        "            await self._send_message(init_message)\n",
        "\n",
        "            # Get available tools\n",
        "            tools_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 2,\n",
        "                \"method\": \"tools/list\",\n",
        "                \"params\": {}\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tools_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                tools = response[\"result\"].get(\"tools\", [])\n",
        "                self.available_tools = []\n",
        "\n",
        "                for tool in tools:\n",
        "                    mcp_tool = MCPTool(\n",
        "                        name=tool.get(\"name\", \"\"),\n",
        "                        description=tool.get(\"description\", \"\"),\n",
        "                        input_schema=tool.get(\"inputSchema\", {})\n",
        "                    )\n",
        "                    self.available_tools.append(mcp_tool)\n",
        "                    logger.info(f\"Loaded MCP tool: {tool.get('name')}\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to connect to MCP server: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def _send_message(self, message: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Send JSON-RPC message to MCP server\"\"\"\n",
        "        if not self.process:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Send message\n",
        "            message_str = json.dumps(message) + \"\\n\"\n",
        "            self.process.stdin.write(message_str)\n",
        "            self.process.stdin.flush()\n",
        "\n",
        "            # Read response\n",
        "            response_str = self.process.stdout.readline()\n",
        "            if response_str:\n",
        "                return json.loads(response_str.strip())\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error communicating with MCP server: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call an MCP tool with given arguments\"\"\"\n",
        "        if not self.process:\n",
        "            raise Exception(\"MCP server not initialized\")\n",
        "\n",
        "        try:\n",
        "            # Send tool call message\n",
        "            tool_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 3,\n",
        "                \"method\": \"tools/call\",\n",
        "                \"params\": {\n",
        "                    \"name\": tool_name,\n",
        "                    \"arguments\": arguments\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tool_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"result\": response[\"result\"],\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"error\": \"No result from MCP server\",\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling MCP tool {tool_name}: {e}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"tool_name\": tool_name\n",
        "            }\n",
        "\n",
        "    def format_tools_for_llm(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Format MCP tools for LLM function calling\"\"\"\n",
        "        formatted_tools = []\n",
        "\n",
        "        for tool in self.available_tools:\n",
        "            formatted_tool = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.input_schema\n",
        "                }\n",
        "            }\n",
        "            formatted_tools.append(formatted_tool)\n",
        "\n",
        "        return formatted_tools\n",
        "\n",
        "    async def call_llm_with_tools(self, messages: List[Dict[str, str]], model: str = \"gpt-4\") -> Dict[str, Any]:\n",
        "        \"\"\"Call LLM with available MCP tools\"\"\"\n",
        "\n",
        "        # Prepare the request payload\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": self.format_tools_for_llm(),\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.llm_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(self.llm_api_url, json=payload, headers=headers) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.json()\n",
        "                else:\n",
        "                    error_text = await response.text()\n",
        "                    raise Exception(f\"LLM API error: {response.status} - {error_text}\")\n",
        "\n",
        "    async def process_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from LLM response\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call[\"function\"][\"name\"]\n",
        "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "\n",
        "            # Call the MCP tool\n",
        "            result = await self.call_mcp_tool(function_name, function_args)\n",
        "\n",
        "            # Format result for LLM\n",
        "            tool_result = {\n",
        "                \"tool_call_id\": tool_call[\"id\"],\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": json.dumps(result)\n",
        "            }\n",
        "            results.append(tool_result)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up MCP server process\"\"\"\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            self.process.wait()\n",
        "            self.process = None\n",
        "\n",
        "    async def chat_with_mcp_tools(self, user_message: str, model: str = \"gpt-4\") -> str:\n",
        "        \"\"\"Complete chat interaction using MCP tools\"\"\"\n",
        "\n",
        "        # Initialize conversation\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to various tools. Use them when needed to answer user questions.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        # Call LLM\n",
        "        llm_response = await self.call_llm_with_tools(messages, model)\n",
        "\n",
        "        # Check if LLM wants to use tools\n",
        "        choice = llm_response[\"choices\"][0]\n",
        "        message = choice[\"message\"]\n",
        "\n",
        "        if message.get(\"tool_calls\"):\n",
        "            # Process tool calls\n",
        "            tool_results = await self.process_tool_calls(message[\"tool_calls\"])\n",
        "\n",
        "            # Add assistant message and tool results to conversation\n",
        "            messages.append(message)\n",
        "            messages.extend(tool_results)\n",
        "\n",
        "            # Call LLM again with tool results\n",
        "            final_response = await self.call_llm_with_tools(messages, model)\n",
        "            return final_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            return message[\"content\"]\n",
        "\n",
        "# Installation and usage example\n",
        "async def main():\n",
        "    # First install required packages\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"aiohttp\"], check=True)\n",
        "\n",
        "    # For proper MCP support, also install:\n",
        "    # subprocess.run([\"pip\", \"install\", \"mcp\"], check=True)\n",
        "\n",
        "    # Configuration\n",
        "    MCP_SERVER_COMMAND = [\"python\", \"-m\", \"simple_mcp_server\"]  # Your MCP server\n",
        "    LLM_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
        "    LLM_API_KEY = \"your-api-key-here\"  # Replace with actual key\n",
        "\n",
        "    # Create connector\n",
        "    connector = MCPLLMConnector(MCP_SERVER_COMMAND, LLM_API_URL, LLM_API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Connect to MCP server\n",
        "        logger.info(\"Connecting to MCP server...\")\n",
        "        success = await connector.connect_mcp_server()\n",
        "\n",
        "        if not success:\n",
        "            logger.error(\"Failed to connect to MCP server\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Connected! Available tools: {[tool.name for tool in connector.available_tools]}\")\n",
        "\n",
        "        # Example chat\n",
        "        user_query = \"What tools are available?\"\n",
        "        if connector.available_tools:\n",
        "            tool_list = \", \".join([tool.name for tool in connector.available_tools])\n",
        "            print(f\"Available MCP tools: {tool_list}\")\n",
        "        else:\n",
        "            print(\"No MCP tools available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "    finally:\n",
        "        # Clean up\n",
        "        connector.cleanup()\n",
        "\n",
        "# Simple standalone MCP-style tool manager (no external dependencies)\n",
        "class SimpleMCPManager:\n",
        "    \"\"\"Simple tool manager that mimics MCP functionality without external deps\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tools = {}\n",
        "        self.setup_default_tools()\n",
        "\n",
        "    def setup_default_tools(self):\n",
        "        \"\"\"Setup some default tools for demonstration\"\"\"\n",
        "        self.tools = {\n",
        "            \"get_time\": {\n",
        "                \"name\": \"get_time\",\n",
        "                \"description\": \"Get current time\",\n",
        "                \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
        "            },\n",
        "            \"calculate\": {\n",
        "                \"name\": \"calculate\",\n",
        "                \"description\": \"Perform basic calculations\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
        "                    },\n",
        "                    \"required\": [\"expression\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def list_tools(self):\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        return list(self.tools.values())\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call a tool with arguments\"\"\"\n",
        "        if tool_name == \"get_time\":\n",
        "            import datetime\n",
        "            return {\"result\": f\"Current time: {datetime.datetime.now()}\"}\n",
        "        elif tool_name == \"calculate\":\n",
        "            try:\n",
        "                expression = arguments.get(\"expression\", \"\")\n",
        "                # Simple safe evaluation (be careful in production!)\n",
        "                result = eval(expression.replace(\"^\", \"**\"))\n",
        "                return {\"result\": f\"{expression} = {result}\"}\n",
        "            except Exception as e:\n",
        "                return {\"error\": f\"Calculation error: {str(e)}\"}\n",
        "        else:\n",
        "            return {\"error\": f\"Unknown tool: {tool_name}\"}\n",
        "\n",
        "# Alternative main function using SimpleMCPManager\n",
        "async def simple_main():\n",
        "    \"\"\"Simple example without external MCP dependencies\"\"\"\n",
        "    manager = SimpleMCPManager()\n",
        "\n",
        "    print(\"Available tools:\")\n",
        "    for tool in manager.list_tools():\n",
        "        print(f\"- {tool['name']}: {tool['description']}\")\n",
        "\n",
        "    # Test tool calls\n",
        "    print(\"\\nTesting tools:\")\n",
        "\n",
        "    time_result = await manager.call_tool(\"get_time\", {})\n",
        "    print(f\"Time tool: {time_result}\")\n",
        "\n",
        "    calc_result = await manager.call_tool(\"calculate\", {\"expression\": \"2 + 2 * 3\"})\n",
        "    print(f\"Calculator tool: {calc_result}\")\n",
        "\n",
        "# For Jupyter/IPython environments (fixes event loop error)\n",
        "def run_async_in_notebook():\n",
        "    \"\"\"Run async functions in Jupyter notebook environment\"\"\"\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "# Check if we're in a notebook environment\n",
        "def is_notebook():\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        return get_ipython() is not None\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Main execution function that works in both environments\n",
        "async def execute_main():\n",
        "    \"\"\"Execute the main function - works in both notebook and script environments\"\"\"\n",
        "    print(\"Choose execution mode:\")\n",
        "    print(\"1. Full MCP integration (requires MCP package)\")\n",
        "    print(\"2. Simple demo without external dependencies\")\n",
        "\n",
        "    # For notebook environments, we'll default to simple demo\n",
        "    if is_notebook():\n",
        "        print(\"Detected notebook environment - running simple demo\")\n",
        "        await simple_main()\n",
        "    else:\n",
        "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
        "        if choice == \"1\":\n",
        "            await main()\n",
        "        else:\n",
        "            await simple_main()\n",
        "\n",
        "# Alternative: Direct execution functions for notebooks\n",
        "async def run_simple_demo():\n",
        "    \"\"\"Run the simple demo directly\"\"\"\n",
        "    await simple_main()\n",
        "\n",
        "async def run_full_mcp():\n",
        "    \"\"\"Run the full MCP integration\"\"\"\n",
        "    await main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if is_notebook():\n",
        "        # In notebook environment, install nest_asyncio to handle event loops\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            # Run the async function directly\n",
        "            import asyncio\n",
        "            asyncio.create_task(execute_main())\n",
        "        except ImportError:\n",
        "            print(\"Installing nest_asyncio for notebook compatibility...\")\n",
        "            import subprocess\n",
        "            subprocess.run([\"pip\", \"install\", \"nest_asyncio\"], check=True)\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            import asyncio\n",
        "            asyncio.create_task(execute_main())\n",
        "    else:\n",
        "        # In regular Python script\n",
        "        asyncio.run(execute_main())\n",
        "\n",
        "# Direct call functions for notebook use\n",
        "def demo():\n",
        "    \"\"\"Easy function to call the demo in notebooks\"\"\"\n",
        "    if is_notebook():\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "        except ImportError:\n",
        "            import subprocess\n",
        "            subprocess.run([\"pip\", \"install\", \"nest_asyncio\"], check=True)\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "\n",
        "        import asyncio\n",
        "        return asyncio.create_task(simple_main())\n",
        "    else:\n",
        "        return asyncio.run(simple_main())\n",
        "\n",
        "# Auto-run demo if in notebook\n",
        "if is_notebook():\n",
        "    print(\"üöÄ Running MCP-LLM Connector Demo...\")\n",
        "    demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7IgEiizunxL",
        "outputId": "f2e1a747-54a5-462c-d804-86d1be7660df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Running MCP-LLM Connector Demo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install the MCP package:\n",
        "# !pip install mcp\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import subprocess\n",
        "from typing import Dict, List, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import aiohttp\n",
        "\n",
        "# Alternative implementation without MCP imports (fallback)\n",
        "# Uncomment the following when MCP is properly installed:\n",
        "# from mcp import ClientSession, StdioServerParameters\n",
        "# from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPTool:\n",
        "    \"\"\"Represents an MCP tool that can be called\"\"\"\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: Dict[str, Any]\n",
        "\n",
        "class MCPLLMConnector:\n",
        "    \"\"\"Connects MCP server with LLM model - Alternative implementation\"\"\"\n",
        "\n",
        "    def __init__(self, server_command: List[str], llm_api_url: str, llm_api_key: str):\n",
        "        self.server_command = server_command\n",
        "        self.llm_api_url = llm_api_url\n",
        "        self.llm_api_key = llm_api_key\n",
        "        self.process: Optional[subprocess.Popen] = None\n",
        "        self.available_tools: List[MCPTool] = []\n",
        "\n",
        "    async def connect_mcp_server(self):\n",
        "        \"\"\"Connect to MCP server using subprocess (fallback method)\"\"\"\n",
        "        try:\n",
        "            # Start MCP server process\n",
        "            self.process = subprocess.Popen(\n",
        "                self.server_command,\n",
        "                stdin=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            # Send initialization message\n",
        "            init_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 1,\n",
        "                \"method\": \"initialize\",\n",
        "                \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}\n",
        "            }\n",
        "\n",
        "            await self._send_message(init_message)\n",
        "\n",
        "            # Get available tools\n",
        "            tools_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 2,\n",
        "                \"method\": \"tools/list\",\n",
        "                \"params\": {}\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tools_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                tools = response[\"result\"].get(\"tools\", [])\n",
        "                self.available_tools = []\n",
        "\n",
        "                for tool in tools:\n",
        "                    mcp_tool = MCPTool(\n",
        "                        name=tool.get(\"name\", \"\"),\n",
        "                        description=tool.get(\"description\", \"\"),\n",
        "                        input_schema=tool.get(\"inputSchema\", {})\n",
        "                    )\n",
        "                    self.available_tools.append(mcp_tool)\n",
        "                    logger.info(f\"Loaded MCP tool: {tool.get('name')}\")\n",
        "\n",
        "                return True\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to connect to MCP server: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def _send_message(self, message: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Send JSON-RPC message to MCP server\"\"\"\n",
        "        if not self.process:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Send message\n",
        "            message_str = json.dumps(message) + \"\\n\"\n",
        "            self.process.stdin.write(message_str)\n",
        "            self.process.stdin.flush()\n",
        "\n",
        "            # Read response\n",
        "            response_str = self.process.stdout.readline()\n",
        "            if response_str:\n",
        "                return json.loads(response_str.strip())\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error communicating with MCP server: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call an MCP tool with given arguments\"\"\"\n",
        "        if not self.process:\n",
        "            raise Exception(\"MCP server not initialized\")\n",
        "\n",
        "        try:\n",
        "            # Send tool call message\n",
        "            tool_message = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 3,\n",
        "                \"method\": \"tools/call\",\n",
        "                \"params\": {\n",
        "                    \"name\": tool_name,\n",
        "                    \"arguments\": arguments\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = await self._send_message(tool_message)\n",
        "\n",
        "            if response and \"result\" in response:\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"result\": response[\"result\"],\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"success\": False,\n",
        "                    \"error\": \"No result from MCP server\",\n",
        "                    \"tool_name\": tool_name\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling MCP tool {tool_name}: {e}\")\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"tool_name\": tool_name\n",
        "            }\n",
        "\n",
        "    def format_tools_for_llm(self) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Format MCP tools for LLM function calling\"\"\"\n",
        "        formatted_tools = []\n",
        "\n",
        "        for tool in self.available_tools:\n",
        "            formatted_tool = {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": tool.name,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.input_schema\n",
        "                }\n",
        "            }\n",
        "            formatted_tools.append(formatted_tool)\n",
        "\n",
        "        return formatted_tools\n",
        "\n",
        "    async def call_llm_with_tools(self, messages: List[Dict[str, str]], model: str = \"gpt-4\") -> Dict[str, Any]:\n",
        "        \"\"\"Call LLM with available MCP tools\"\"\"\n",
        "\n",
        "        # Prepare the request payload\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": self.format_tools_for_llm(),\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.llm_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.post(self.llm_api_url, json=payload, headers=headers) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.json()\n",
        "                else:\n",
        "                    error_text = await response.text()\n",
        "                    raise Exception(f\"LLM API error: {response.status} - {error_text}\")\n",
        "\n",
        "    async def process_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process tool calls from LLM response\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            function_name = tool_call[\"function\"][\"name\"]\n",
        "            function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
        "\n",
        "            # Call the MCP tool\n",
        "            result = await self.call_mcp_tool(function_name, function_args)\n",
        "\n",
        "            # Format result for LLM\n",
        "            tool_result = {\n",
        "                \"tool_call_id\": tool_call[\"id\"],\n",
        "                \"role\": \"tool\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": json.dumps(result)\n",
        "            }\n",
        "            results.append(tool_result)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up MCP server process\"\"\"\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            self.process.wait()\n",
        "            self.process = None\n",
        "\n",
        "    async def chat_with_mcp_tools(self, user_message: str, model: str = \"gpt-4\") -> str:\n",
        "        \"\"\"Complete chat interaction using MCP tools\"\"\"\n",
        "\n",
        "        # Initialize conversation\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to various tools. Use them when needed to answer user questions.\"},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ]\n",
        "\n",
        "        # Call LLM\n",
        "        llm_response = await self.call_llm_with_tools(messages, model)\n",
        "\n",
        "        # Check if LLM wants to use tools\n",
        "        choice = llm_response[\"choices\"][0]\n",
        "        message = choice[\"message\"]\n",
        "\n",
        "        if message.get(\"tool_calls\"):\n",
        "            # Process tool calls\n",
        "            tool_results = await self.process_tool_calls(message[\"tool_calls\"])\n",
        "\n",
        "            # Add assistant message and tool results to conversation\n",
        "            messages.append(message)\n",
        "            messages.extend(tool_results)\n",
        "\n",
        "            # Call LLM again with tool results\n",
        "            final_response = await self.call_llm_with_tools(messages, model)\n",
        "            return final_response[\"choices\"][0][\"message\"][\"content\"]\n",
        "        else:\n",
        "            return message[\"content\"]\n",
        "\n",
        "# Installation and usage example\n",
        "async def main():\n",
        "    # First install required packages\n",
        "    print(\"Installing required packages...\")\n",
        "    subprocess.run([\"pip\", \"install\", \"aiohttp\"], check=True)\n",
        "\n",
        "    # For proper MCP support, also install:\n",
        "    # subprocess.run([\"pip\", \"install\", \"mcp\"], check=True)\n",
        "\n",
        "    # Configuration\n",
        "    MCP_SERVER_COMMAND = [\"python\", \"-m\", \"simple_mcp_server\"]  # Your MCP server\n",
        "    LLM_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
        "    LLM_API_KEY = \"your-api-key-here\"  # Replace with actual key\n",
        "\n",
        "    # Create connector\n",
        "    connector = MCPLLMConnector(MCP_SERVER_COMMAND, LLM_API_URL, LLM_API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Connect to MCP server\n",
        "        logger.info(\"Connecting to MCP server...\")\n",
        "        success = await connector.connect_mcp_server()\n",
        "\n",
        "        if not success:\n",
        "            logger.error(\"Failed to connect to MCP server\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Connected! Available tools: {[tool.name for tool in connector.available_tools]}\")\n",
        "\n",
        "        # Example chat\n",
        "        user_query = \"What tools are available?\"\n",
        "        if connector.available_tools:\n",
        "            tool_list = \", \".join([tool.name for tool in connector.available_tools])\n",
        "            print(f\"Available MCP tools: {tool_list}\")\n",
        "        else:\n",
        "            print(\"No MCP tools available\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "    finally:\n",
        "        # Clean up\n",
        "        connector.cleanup()\n",
        "\n",
        "# Simple standalone MCP-style tool manager (no external dependencies)\n",
        "class SimpleMCPManager:\n",
        "    \"\"\"Simple tool manager that mimics MCP functionality without external deps\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tools = {}\n",
        "        self.setup_default_tools()\n",
        "\n",
        "    def setup_default_tools(self):\n",
        "        \"\"\"Setup some default tools for demonstration\"\"\"\n",
        "        self.tools = {\n",
        "            \"get_time\": {\n",
        "                \"name\": \"get_time\",\n",
        "                \"description\": \"Get current time\",\n",
        "                \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
        "            },\n",
        "            \"calculate\": {\n",
        "                \"name\": \"calculate\",\n",
        "                \"description\": \"Perform basic calculations\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"expression\": {\"type\": \"string\", \"description\": \"Math expression to evaluate\"}\n",
        "                    },\n",
        "                    \"required\": [\"expression\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def list_tools(self):\n",
        "        \"\"\"List available tools\"\"\"\n",
        "        return list(self.tools.values())\n",
        "\n",
        "    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Call a tool with arguments\"\"\"\n",
        "        if tool_name == \"get_time\":\n",
        "            import datetime\n",
        "            return {\"result\": f\"Current time: {datetime.datetime.now()}\"}\n",
        "        elif tool_name == \"calculate\":\n",
        "            try:\n",
        "                expression = arguments.get(\"expression\", \"\")\n",
        "                # Simple safe evaluation (be careful in production!)\n",
        "                result = eval(expression.replace(\"^\", \"**\"))\n",
        "                return {\"result\": f\"{expression} = {result}\"}\n",
        "            except Exception as e:\n",
        "                return {\"error\": f\"Calculation error: {str(e)}\"}\n",
        "        else:\n",
        "            return {\"error\": f\"Unknown tool: {tool_name}\"}\n",
        "\n",
        "# Alternative main function using SimpleMCPManager\n",
        "async def simple_main():\n",
        "    \"\"\"Simple example without external MCP dependencies\"\"\"\n",
        "    manager = SimpleMCPManager()\n",
        "\n",
        "    print(\"Available tools:\")\n",
        "    for tool in manager.list_tools():\n",
        "        print(f\"- {tool['name']}: {tool['description']}\")\n",
        "\n",
        "    # Test tool calls\n",
        "    print(\"\\nTesting tools:\")\n",
        "\n",
        "    time_result = await manager.call_tool(\"get_time\", {})\n",
        "    print(f\"Time tool: {time_result}\")\n",
        "\n",
        "    calc_result = await manager.call_tool(\"calculate\", {\"expression\": \"2 + 2 * 3\"})\n",
        "    print(f\"Calculator tool: {calc_result}\")\n",
        "\n",
        "# For Jupyter/IPython environments (fixes event loop error)\n",
        "def run_async_in_notebook():\n",
        "    \"\"\"Run async functions in Jupyter notebook environment\"\"\"\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "# Check if we're in a notebook environment\n",
        "def is_notebook():\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        return get_ipython() is not None\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "# Main execution function that works in both environments\n",
        "async def execute_main():\n",
        "    \"\"\"Execute the main function - works in both notebook and script environments\"\"\"\n",
        "    print(\"Choose execution mode:\")\n",
        "    print(\"1. Full MCP integration (requires MCP package)\")\n",
        "    print(\"2. Simple demo without external dependencies\")\n",
        "\n",
        "    # For notebook environments, we'll default to simple demo\n",
        "    if is_notebook():\n",
        "        print(\"Detected notebook environment - running simple demo\")\n",
        "        await simple_main()\n",
        "    else:\n",
        "        choice = input(\"Enter choice (1 or 2): \").strip()\n",
        "        if choice == \"1\":\n",
        "            await main()\n",
        "        else:\n",
        "            await simple_main()\n",
        "\n",
        "# Alternative: Direct execution functions for notebooks\n",
        "async def run_simple_demo():\n",
        "    \"\"\"Run the simple demo directly\"\"\"\n",
        "    await simple_main()\n",
        "\n",
        "async def run_full_mcp():\n",
        "    \"\"\"Run the full MCP integration\"\"\"\n",
        "    await main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if is_notebook():\n",
        "        # In notebook environment, install nest_asyncio to handle event loops\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            # Run the async function directly\n",
        "            import asyncio\n",
        "            asyncio.create_task(execute_main())\n",
        "        except ImportError:\n",
        "            print(\"Installing nest_asyncio for notebook compatibility...\")\n",
        "            import subprocess\n",
        "            subprocess.run([\"pip\", \"install\", \"nest_asyncio\"], check=True)\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            import asyncio\n",
        "            asyncio.create_task(execute_main())\n",
        "    else:\n",
        "        # In regular Python script\n",
        "        asyncio.run(execute_main())\n",
        "\n",
        "# Direct call functions for notebook use\n",
        "def demo():\n",
        "    \"\"\"Easy function to call the demo in notebooks\"\"\"\n",
        "    if is_notebook():\n",
        "        try:\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "        except ImportError:\n",
        "            import subprocess\n",
        "            subprocess.run([\"pip\", \"install\", \"nest_asyncio\"], check=True)\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "\n",
        "        import asyncio\n",
        "        return asyncio.create_task(simple_main())\n",
        "    else:\n",
        "        return asyncio.run(simple_main())"
      ],
      "metadata": {
        "id": "09Kz6qJEuzrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JPyQ3DgdE_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def initialize_mcp():\n",
        "    print(\"üîå Connecting to MCP server...\")\n",
        "    success = await connector.connect_mcp_server()\n",
        "    if success:\n",
        "        print(\"‚úÖ MCP connected. Tools loaded:\", [tool.name for tool in connector.available_tools])\n",
        "    else:\n",
        "        print(\"‚ùå MCP connection failed.\")\n"
      ],
      "metadata": {
        "id": "w6Pd03CIdWzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip show langchain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtqoH0afqGQC",
        "outputId": "20e6be31-6dd3-44ef-d57f-0cfc4da2c271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.26\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: langchain-community\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n"
      ],
      "metadata": {
        "id": "xls1P3BeqZ8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import ToolMessage\n"
      ],
      "metadata": {
        "id": "l7bMM2scrJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mcp_langchain_connector.py\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "from langchain.chat_models.base import BaseChatModel\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class MCPTool:\n",
        "    name: str\n",
        "    description: str\n",
        "    input_schema: Dict[str, Any]\n",
        "\n",
        "class MCPLLMConnectorLangChain:\n",
        "    def __init__(self, server_command: List[str], llm: BaseChatModel):\n",
        "        self.server_command = server_command\n",
        "        self.llm = llm\n",
        "        self.process: Optional[subprocess.Popen] = None\n",
        "        self.available_tools: List[MCPTool] = []\n",
        "\n",
        "    async def connect_mcp_server(self) -> bool:\n",
        "        try:\n",
        "            self.process = subprocess.Popen(\n",
        "                self.server_command,\n",
        "                stdin=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            # Init\n",
        "            init_msg = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 1,\n",
        "                \"method\": \"initialize\",\n",
        "                \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}\n",
        "            }\n",
        "            await self._send(init_msg)\n",
        "\n",
        "            # Get tools\n",
        "            tool_list_msg = {\n",
        "                \"jsonrpc\": \"2.0\",\n",
        "                \"id\": 2,\n",
        "                \"method\": \"tools/list\",\n",
        "                \"params\": {}\n",
        "            }\n",
        "            response = await self._send(tool_list_msg)\n",
        "            if response and \"result\" in response:\n",
        "                tools = response[\"result\"].get(\"tools\", [])\n",
        "                self.available_tools = [\n",
        "                    MCPTool(\n",
        "                        name=t[\"name\"],\n",
        "                        description=t[\"description\"],\n",
        "                        input_schema=t[\"inputSchema\"]\n",
        "                    ) for t in tools\n",
        "                ]\n",
        "                return True\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP server connection failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def _send(self, message: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "        try:\n",
        "            self.process.stdin.write(json.dumps(message) + \"\\n\")\n",
        "            self.process.stdin.flush()\n",
        "            response = self.process.stdout.readline()\n",
        "            if response:\n",
        "                return json.loads(response.strip())\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP comm error: {e}\")\n",
        "        return None\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        tool_msg = {\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 3,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\"name\": tool_name, \"arguments\": arguments}\n",
        "        }\n",
        "        response = await self._send(tool_msg)\n",
        "        if response and \"result\" in response:\n",
        "            return response[\"result\"]\n",
        "        else:\n",
        "            return {\"error\": \"No result\"}\n",
        "\n",
        "    def format_tools_for_langchain(self):\n",
        "        from langchain.tools import Tool\n",
        "        tools = []\n",
        "        for t in self.available_tools:\n",
        "            async def _tool_func(*, _tool=t):  # bind each tool\n",
        "                def _wrapped_func(**kwargs):\n",
        "                    return asyncio.run(self.call_mcp_tool(_tool.name, kwargs))\n",
        "                return _wrapped_func\n",
        "\n",
        "            tools.append(Tool.from_function(\n",
        "                name=t.name,\n",
        "                description=t.description,\n",
        "                args_schema=None,  # Use input_schema manually if needed\n",
        "                func=_tool_func()\n",
        "            ))\n",
        "        return tools\n",
        "\n",
        "    async def chat_with_tools(self, user_msg: str) -> str:\n",
        "        messages = [\n",
        "            SystemMessage(content=\"You are a tool-enabled mental health assistant.\"),\n",
        "            HumanMessage(content=user_msg)\n",
        "        ]\n",
        "        response = self.llm(messages)\n",
        "        return response.content\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            self.process.wait()\n"
      ],
      "metadata": {
        "id": "JYswTlmuoezX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mcp_langchain_connector.py\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "class MCPLLMConnectorLangChain:\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        self.llm = llm\n",
        "\n",
        "    def call_with_tools(self, messages: List[Dict[str, str]], tools: List[Dict[str, Any]]) -> Any:\n",
        "        # Sample method stub\n",
        "        print(\"Calling with tools...\")\n",
        "        return self.llm(messages)\n"
      ],
      "metadata": {
        "id": "iP4pWOXzr6Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [{\n",
        "    \"name\": \"calculate\",\n",
        "    \"description\": \"Perform a math calculation.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"expression\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Math expression like 2+2 or 5*10\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"expression\"]\n",
        "    }\n",
        "}]\n"
      ],
      "metadata": {
        "id": "jL60pfmowdun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mcp_langchain_connector.py\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "class MCPLLMConnectorLangChain:\n",
        "    def __init__(self, llm: ChatOpenAI, available_tools: List[Dict[str, Any]]):\n",
        "        self.llm = llm\n",
        "        self.available_tools = available_tools\n",
        "\n",
        "    def format_tools(self):\n",
        "        \"\"\"Format tools for OpenAI function calling style\"\"\"\n",
        "        return [{\"type\": \"function\", \"function\": tool} for tool in self.available_tools]\n",
        "\n",
        "    def call_with_tools(self, user_input: str) -> Dict[str, Any]:\n",
        "        \"\"\"Call the LLM with tools enabled\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a smart assistant with access to tools.\"},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ]\n",
        "        response = self.llm.invoke({\n",
        "            \"messages\": messages,\n",
        "            \"tools\": self.format_tools(),\n",
        "            \"tool_choice\": \"auto\"\n",
        "        })\n",
        "        return response\n"
      ],
      "metadata": {
        "id": "0AbINPr1xKQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "import logging\n",
        "from typing import Dict, List, Any, Optional\n",
        "from langchain.schema.messages import AIMessage, HumanMessage, ToolMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MCPLLMConnectorLangChain:\n",
        "    def __init__(self, server_command: List[str], llm_api_key: str, model=\"gpt-4\"):\n",
        "        self.server_command = server_command\n",
        "        self.api_key = llm_api_key\n",
        "        self.model = model\n",
        "        self.process = None\n",
        "        self.available_tools = []\n",
        "        self.llm = None\n",
        "\n",
        "    def start_server(self):\n",
        "        self.process = subprocess.Popen(\n",
        "            self.server_command,\n",
        "            stdin=subprocess.PIPE,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        self._send({\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 1,\n",
        "            \"method\": \"initialize\",\n",
        "            \"params\": {\"protocolVersion\": \"2024-11-05\", \"capabilities\": {}}\n",
        "        })\n",
        "\n",
        "        response = self._send({\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 2,\n",
        "            \"method\": \"tools/list\",\n",
        "            \"params\": {}\n",
        "        })\n",
        "\n",
        "        if response and \"result\" in response:\n",
        "            self.available_tools = response[\"result\"][\"tools\"]\n",
        "            logger.info(f\"‚úÖ MCP tools loaded: {[t['name'] for t in self.available_tools]}\")\n",
        "\n",
        "        return self.available_tools\n",
        "\n",
        "    def _send(self, msg: Dict[str, Any]):\n",
        "        try:\n",
        "            self.process.stdin.write(json.dumps(msg) + \"\\n\")\n",
        "            self.process.stdin.flush()\n",
        "            return json.loads(self.process.stdout.readline())\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MCP send error: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def call_tool(self, name: str, arguments: Dict[str, Any]):\n",
        "        response = self._send({\n",
        "            \"jsonrpc\": \"2.0\",\n",
        "            \"id\": 3,\n",
        "            \"method\": \"tools/call\",\n",
        "            \"params\": {\n",
        "                \"name\": name,\n",
        "                \"arguments\": arguments\n",
        "            }\n",
        "        })\n",
        "        return response.get(\"result\", {})\n",
        "\n",
        "    def format_tools(self):\n",
        "        \"\"\"Format tools for OpenAI function calling (via LangChain)\"\"\"\n",
        "        return [{\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": tool[\"name\"],\n",
        "                \"description\": tool.get(\"description\", \"\"),\n",
        "                \"parameters\": tool.get(\"inputSchema\", {})\n",
        "            }\n",
        "        } for tool in self.available_tools]\n",
        "\n",
        "    def setup_llm(self):\n",
        "        self.llm = ChatOpenAI(\n",
        "            model_name=self.model,\n",
        "            temperature=0.3,\n",
        "            openai_api_key=self.api_key,\n",
        "            tools=self.format_tools()\n",
        "        )\n",
        "        return self.llm\n"
      ],
      "metadata": {
        "id": "g5EmyTEByk0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def connect_and_run_mcp():\n",
        "    # Your MCP server command (replace with actual)\n",
        "    MCP_SERVER_COMMAND = [\"python\", \"-m\", \"simple_mcp_server\"]\n",
        "\n",
        "    # Use your existing LLM instance\n",
        "    connector = MCPLLMConnectorLangChain(server_command=MCP_SERVER_COMMAND, llm=llm)\n",
        "\n",
        "    print(\"üîå Connecting to MCP server...\")\n",
        "    connected = await connector.connect_mcp_server()\n",
        "\n",
        "    if not connected:\n",
        "        print(\"‚ùå Failed to connect to MCP server.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚úÖ Connected to MCP. Available tools:\")\n",
        "    for tool in connector.available_tools:\n",
        "        print(f\"üõ†Ô∏è {tool.name}: {tool.description}\")\n",
        "\n",
        "    # Example usage (optional demo call)\n",
        "    if connector.available_tools:\n",
        "        result = await connector.call_mcp_tool(connector.available_tools[0].name, {})\n",
        "        print(\"üîç Example Tool Output:\", result)\n",
        "\n",
        "    # Cleanup MCP process\n",
        "    connector.cleanup()\n"
      ],
      "metadata": {
        "id": "znraen0C0M2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    try:\n",
        "        chat_msgs = extract_whatsapp_messages(whatsapp_file)\n",
        "        chat_result = analyze_chat(chat_msgs) if chat_msgs else \"No messages\"\n",
        "\n",
        "        screen_df = load_screen_time(screen_time_file)\n",
        "        screen_result = analyze_screen_time(screen_df) if not screen_df.empty else \"No screen data\"\n",
        "\n",
        "        tweets_df = pd.read_csv(twitter_file)\n",
        "        sentiment_df = analyze_tweets(tweets_df)\n",
        "\n",
        "        final_report = synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "        print(\"\\nüß† Final Teen Mental Health Summary:\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "        # üîó Connect to MCP after full report (optional)\n",
        "        asyncio.run(connect_and_run_mcp())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Pipeline Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq0cmk970WBl",
        "outputId": "b58e9450-cce5-4626-ea82-4cfbf74310d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Failed to extract chat: ZipFile requires mode 'r', 'w', 'x', or 'a'\n",
            "‚ùå Pipeline Error: [Errno 2] No such file or directory: 'screentime_analysis.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWh4Nqx9wtmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mental Health LLM Client with MCP Integration\n",
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "from typing import Any, Dict, List, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Install required packages:\n",
        "# pip install mcp openai anthropic python-dotenv\n",
        "\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class MentalHealthLLMClient:\n",
        "    def __init__(self, openai_api_key: str = None, anthropic_api_key: str = None):\n",
        "        # Initialize LLM clients\n",
        "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "        self.anthropic_api_key = anthropic_api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "        if self.openai_api_key:\n",
        "            openai.api_key = self.openai_api_key\n",
        "            self.llm = ChatOpenAI(\n",
        "                model_name=\"gpt-4\",\n",
        "                temperature=0.3,\n",
        "                openai_api_key=self.openai_api_key\n",
        "            )\n",
        "\n",
        "        if self.anthropic_api_key:\n",
        "            import anthropic\n",
        "            self.anthropic_client = anthropic.Anthropic(api_key=self.anthropic_api_key)\n",
        "\n",
        "        # MCP connection\n",
        "        self.mcp_session: Optional[ClientSession] = None\n",
        "        self.available_tools = []\n",
        "        self.available_resources = []\n",
        "\n",
        "    async def connect_to_mcp_server(self, server_script_path: str = \"mental_health_mcp_server.py\"):\n",
        "        \"\"\"Connect to the Mental Health MCP server\"\"\"\n",
        "        try:\n",
        "            server_params = StdioServerParameters(\n",
        "                command=\"python\",\n",
        "                args=[server_script_path]\n",
        "            )\n",
        "\n",
        "            stdio_transport = stdio_client(server_params)\n",
        "            stdio, write = await stdio_transport.__aenter__()\n",
        "\n",
        "            self.mcp_session = ClientSession(stdio, write)\n",
        "            await self.mcp_session.initialize()\n",
        "\n",
        "            # Get available tools and resources\n",
        "            tools_result = await self.mcp_session.list_tools()\n",
        "            self.available_tools = tools_result.tools\n",
        "\n",
        "            resources_result = await self.mcp_session.list_resources()\n",
        "            self.available_resources = resources_result.resources\n",
        "\n",
        "            print(f\"‚úÖ Connected to Mental Health MCP Server\")\n",
        "            print(f\"üìã Available tools: {[tool.name for tool in self.available_tools]}\")\n",
        "            print(f\"üìö Available resources: {[resource.name for resource in self.available_resources]}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error connecting to MCP server: {e}\")\n",
        "            return False\n",
        "\n",
        "    async def call_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> str:\n",
        "        \"\"\"Call an MCP tool and return the result\"\"\"\n",
        "        if not self.mcp_session:\n",
        "            return \"Error: Not connected to MCP server\"\n",
        "\n",
        "        try:\n",
        "            result = await self.mcp_session.call_tool(tool_name, arguments)\n",
        "\n",
        "            if result.content and len(result.content) > 0:\n",
        "                return result.content[0].text\n",
        "            else:\n",
        "                return \"No content returned from MCP tool\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error calling MCP tool {tool_name}: {str(e)}\"\n",
        "\n",
        "    async def get_mcp_resource(self, resource_uri: str) -> str:\n",
        "        \"\"\"Get content from an MCP resource\"\"\"\n",
        "        if not self.mcp_session:\n",
        "            return \"Error: Not connected to MCP server\"\n",
        "\n",
        "        try:\n",
        "            result = await self.mcp_session.read_resource(resource_uri)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            return f\"Error reading MCP resource {resource_uri}: {str(e)}\"\n",
        "\n",
        "    async def analyze_mental_health_complete(self,\n",
        "                                           whatsapp_zip_path: str = None,\n",
        "                                           screen_time_csv_path: str = None,\n",
        "                                           social_posts: List[str] = None,\n",
        "                                           use_llm_enhancement: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Complete mental health analysis using MCP tools and LLM enhancement\"\"\"\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Step 1: Extract WhatsApp messages if provided\n",
        "        if whatsapp_zip_path:\n",
        "            print(\"üì± Extracting WhatsApp messages...\")\n",
        "            whatsapp_result = await self.call_mcp_tool(\n",
        "                \"extract_whatsapp_messages\",\n",
        "                {\"zip_path\": whatsapp_zip_path}\n",
        "            )\n",
        "            whatsapp_data = json.loads(whatsapp_result)\n",
        "            results[\"whatsapp_extraction\"] = whatsapp_data\n",
        "\n",
        "            # Analyze chat sentiment\n",
        "            if \"messages\" in whatsapp_data:\n",
        "                print(\"üí≠ Analyzing chat sentiment...\")\n",
        "                chat_analysis = await self.call_mcp_tool(\n",
        "                    \"analyze_chat_sentiment\",\n",
        "                    {\"messages\": whatsapp_data[\"messages\"][:50]}  # Analyze recent 50 messages\n",
        "                )\n",
        "                results[\"chat_analysis\"] = json.loads(chat_analysis)\n",
        "\n",
        "        # Step 2: Load and analyze screen time data\n",
        "        if screen_time_csv_path:\n",
        "            print(\"üìä Loading screen time data...\")\n",
        "            screen_data_result = await self.call_mcp_tool(\n",
        "                \"load_screen_time_data\",\n",
        "                {\"csv_path\": screen_time_csv_path}\n",
        "            )\n",
        "            screen_data = json.loads(screen_data_result)\n",
        "            results[\"screen_data\"] = screen_data\n",
        "\n",
        "            # Analyze digital wellness\n",
        "            print(\"üñ•Ô∏è Analyzing digital wellness...\")\n",
        "            wellness_analysis = await self.call_mcp_tool(\n",
        "                \"analyze_digital_wellness\",\n",
        "                {\"screen_time_data\": screen_data_result}\n",
        "            )\n",
        "            results[\"wellness_analysis\"] = json.loads(wellness_analysis)\n",
        "\n",
        "        # Step 3: Analyze social media sentiment\n",
        "        if social_posts:\n",
        "            print(\"üåê Analyzing social media sentiment...\")\n",
        "            sentiment_analysis = await self.call_mcp_tool(\n",
        "                \"analyze_social_sentiment\",\n",
        "                {\"posts\": social_posts, \"platform\": \"twitter\"}\n",
        "            )\n",
        "            results[\"sentiment_analysis\"] = json.loads(sentiment_analysis)\n",
        "\n",
        "        # Step 4: Generate comprehensive report\n",
        "        if len(results) >= 2:  # Need at least 2 data sources\n",
        "            print(\"üìã Generating comprehensive mental health report...\")\n",
        "\n",
        "            chat_analysis_json = json.dumps(results.get(\"chat_analysis\", {}))\n",
        "            screen_analysis_json = json.dumps(results.get(\"wellness_analysis\", {}))\n",
        "            sentiment_analysis_json = json.dumps(results.get(\"sentiment_analysis\", {}))\n",
        "\n",
        "            report_result = await self.call_mcp_tool(\n",
        "                \"generate_mental_health_report\",\n",
        "                {\n",
        "                    \"chat_analysis\": chat_analysis_json,\n",
        "                    \"screen_analysis\": screen_analysis_json,\n",
        "                    \"sentiment_analysis\": sentiment_analysis_json,\n",
        "                    \"report_type\": \"detailed\"\n",
        "                }\n",
        "            )\n",
        "            results[\"comprehensive_report\"] = json.loads(report_result)\n",
        "\n",
        "            # Step 5: LLM Enhancement (if enabled)\n",
        "            if use_llm_enhancement and hasattr(self, 'llm'):\n",
        "                print(\"üß† Enhancing report with LLM insights...\")\n",
        "                enhanced_report = await self.enhance_report_with_llm(results[\"comprehensive_report\"])\n",
        "                results[\"llm_enhanced_insights\"] = enhanced_report\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def enhance_report_with_llm(self, base_report: Dict[str, Any]) -> str:\n",
        "        \"\"\"Use LLM to provide additional insights and personalized recommendations\"\"\"\n",
        "\n",
        "        # Get analysis templates from MCP resources\n",
        "        templates = await self.get_mcp_resource(\"mental-health://analysis/templates\")\n",
        "        recommendations_db = await self.get_mcp_resource(\"mental-health://recommendations/database\")\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"report\", \"templates\", \"recommendations\"],\n",
        "            template=\"\"\"\n",
        "You are an expert mental health AI therapist with access to comprehensive digital behavior analysis.\n",
        "\n",
        "Based on this mental health analysis report: {report}\n",
        "\n",
        "Using these analysis templates: {templates}\n",
        "\n",
        "And this recommendations database: {recommendations}\n",
        "\n",
        "Please provide:\n",
        "\n",
        "1. **Personalized Insights**: What are the top 3 most important patterns you notice?\n",
        "\n",
        "2. **Actionable Recommendations**: Give 5 specific, achievable daily actions this person can take immediately.\n",
        "\n",
        "3. **Content Therapy**: Recommend 3 movies, 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "iZaaOMLAwtR2",
        "outputId": "a1d5c5cf-e632-42db-fa4d-8e3cf8b8dd33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-9-4198351853.py, line 192)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-9-4198351853.py\"\u001b[0;36m, line \u001b[0;32m192\u001b[0m\n\u001b[0;31m    template=\"\"\"\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kp5N58xRzR_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mcp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdjYFGbf2NCP",
        "outputId": "382484a4-370e-4a41-cfbc-0f1af2243cb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mcp\n",
            "  Downloading mcp-1.9.4-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.11/dist-packages (from mcp) (4.9.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.11/dist-packages (from mcp) (0.4.1)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp) (0.28.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from mcp) (2.10.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from mcp) (2.11.7)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp) (0.0.20)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp)\n",
            "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp) (0.46.2)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from mcp) (0.34.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp) (4.14.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->mcp) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->mcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->mcp) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.2->mcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.2->mcp) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.5.2->mcp) (1.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.1->mcp) (8.2.1)\n",
            "Downloading mcp-1.9.4-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: sse-starlette, mcp\n",
            "Successfully installed mcp-1.9.4 sse-starlette-2.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import asyncio\n",
        "from typing import Dict, List, Any, Optional\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# MCP imports\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "class MCPLLMClient:\n",
        "    \"\"\"MCP-based LLM client for mental health analysis\"\"\"\n",
        "\n",
        "    def __init__(self, server_command: List[str], model_name: str = \"claude-3-sonnet\"):\n",
        "        self.server_command = server_command\n",
        "        self.model_name = model_name\n",
        "        self.session: Optional[ClientSession] = None\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        \"\"\"Async context manager entry\"\"\"\n",
        "        server_params = StdioServerParameters(\n",
        "            command=self.server_command[0],\n",
        "            args=self.server_command[1:] if len(self.server_command) > 1 else [],\n",
        "            env=None\n",
        "        )\n",
        "\n",
        "        self.stdio_client = stdio_client(server_params)\n",
        "        self.stdio, self.write = await self.stdio_client.__aenter__()\n",
        "        self.session = await ClientSession(self.stdio, self.write).__aenter__()\n",
        "\n",
        "        # Initialize the session\n",
        "        await self.session.initialize()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Async context manager exit\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.__aexit__(exc_type, exc_val, exc_tb)\n",
        "        await self.stdio_client.__aexit__(exc_type, exc_val, exc_tb)\n",
        "\n",
        "    async def generate_response(self, prompt: str, max_tokens: int = 2000) -> str:\n",
        "        \"\"\"Generate response using MCP model\"\"\"\n",
        "        try:\n",
        "            # Create a sampling request\n",
        "            result = await self.session.create_message(\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt\n",
        "                    }\n",
        "                }],\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=0.5\n",
        "            )\n",
        "\n",
        "            return result.content[0].text if result.content else \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå MCP generation error: {e}\")\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "class MentalHealthAnalyzer:\n",
        "    \"\"\"Mental health analysis using MCP models\"\"\"\n",
        "\n",
        "    def __init__(self, mcp_server_command: List[str]):\n",
        "        self.mcp_server_command = mcp_server_command\n",
        "\n",
        "    async def extract_whatsapp_messages(self, zip_path: str) -> List[str]:\n",
        "        \"\"\"Extract WhatsApp messages from zip file\"\"\"\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                txt_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "                if not txt_files:\n",
        "                    return []\n",
        "                with zip_ref.open(txt_files[0]) as f:\n",
        "                    try:\n",
        "                        chat_data = f.read().decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        chat_data = f.read().decode('latin1')\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error extracting chat: {e}\")\n",
        "            return []\n",
        "\n",
        "        lines = chat_data.split('\\n')\n",
        "        merged_lines = []\n",
        "        date_pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ')\n",
        "        buffer = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            if date_pattern.match(line):\n",
        "                if buffer:\n",
        "                    merged_lines.append(buffer)\n",
        "                buffer = line\n",
        "            else:\n",
        "                buffer += \" \" + line.strip()\n",
        "        if buffer:\n",
        "            merged_lines.append(buffer)\n",
        "\n",
        "        pattern = re.compile(r'^\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2} [APMapm]{2} - ([^:]+): (.+)$')\n",
        "        messages = [\n",
        "            f\"{m.group(1)}: {m.group(2)}\"\n",
        "            for m in map(pattern.match, merged_lines)\n",
        "            if m and \"media omitted\" not in m.group(2).lower()\n",
        "        ]\n",
        "        return messages\n",
        "\n",
        "    async def analyze_chat(self, messages: List[str], n: int = 50) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze chat messages using MCP\"\"\"\n",
        "        recent = \"\\n\".join(messages[-n:])\n",
        "\n",
        "        prompt = f\"\"\"You are a futuristic AI therapist from 2030.\n",
        "\n",
        "Analyze these WhatsApp messages:\n",
        "- Emotional tone (stress, joy, anxiety)\n",
        "- Mental clarity & decision style\n",
        "- Mindset type: proactive, reactive, balanced\n",
        "\n",
        "Recommend:\n",
        "- 3 apps/habits to avoid\n",
        "- 3 uplifting movies and songs\n",
        "- 3 good daily mental health habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{{\"emotional_tone\": \"...\", \"clarity\": \"...\", \"mindset\": \"...\", \"avoid\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...] }}\n",
        "\n",
        "Chat:\n",
        "{recent}\"\"\"\n",
        "\n",
        "        async with MCPLLMClient(self.mcp_server_command) as llm:\n",
        "            response = await llm.generate_response(prompt)\n",
        "\n",
        "        try:\n",
        "            # Extract JSON from response\n",
        "            json_start = response.find('{')\n",
        "            json_end = response.rfind('}') + 1\n",
        "            if json_start != -1 and json_end != 0:\n",
        "                json_str = response[json_start:json_end]\n",
        "                return json.loads(json_str)\n",
        "            else:\n",
        "                return json.loads(response)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è Chat JSON parse error: {e}\")\n",
        "            print(f\"Response: {response}\")\n",
        "            return {\n",
        "                \"emotional_tone\": \"Unable to analyze\",\n",
        "                \"clarity\": \"Error in analysis\",\n",
        "                \"mindset\": \"Unknown\",\n",
        "                \"avoid\": [],\n",
        "                \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "                \"habits\": []\n",
        "            }\n",
        "\n",
        "    def load_screen_time(self, csv_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load screen time data\"\"\"\n",
        "        try:\n",
        "            return pd.read_csv(csv_path)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load screen time CSV: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    async def analyze_screen_time(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze screen time data using MCP\"\"\"\n",
        "        if df.empty:\n",
        "            return {\n",
        "                \"clarity_score\": 50,\n",
        "                \"fatigue\": \"No data available\",\n",
        "                \"avoid_apps\": [],\n",
        "                \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "                \"habits\": []\n",
        "            }\n",
        "\n",
        "        readable = df.to_string(index=False)\n",
        "\n",
        "        prompt = f\"\"\"You are a digital wellness AI.\n",
        "\n",
        "Analyze this screen time data:\n",
        "- Focus vs distraction\n",
        "- Burnout, overuse, addiction\n",
        "- Decision fatigue signs\n",
        "\n",
        "Recommend:\n",
        "- Mental clarity (0-100)\n",
        "- Avoid apps\n",
        "- 3 inspiring movies and calming songs\n",
        "- 3 digital detox habits\n",
        "\n",
        "Output ONLY in JSON format:\n",
        "{{\"clarity_score\": 0, \"fatigue\": \"...\", \"avoid_apps\": [...], \"recommend\": {{\"movies\": [...], \"songs\": [...]}}, \"habits\": [...] }}\n",
        "\n",
        "Screen Time Data:\n",
        "{readable}\"\"\"\n",
        "\n",
        "        async with MCPLLMClient(self.mcp_server_command) as llm:\n",
        "            response = await llm.generate_response(prompt)\n",
        "\n",
        "        try:\n",
        "            json_start = response.find('{')\n",
        "            json_end = response.rfind('}') + 1\n",
        "            if json_start != -1 and json_end != 0:\n",
        "                json_str = response[json_start:json_end]\n",
        "                data = json.loads(json_str)\n",
        "            else:\n",
        "                data = json.loads(response)\n",
        "\n",
        "            data[\"clarity_score\"] = max(0, min(100, int(data.get(\"clarity_score\", 50))))\n",
        "            return data\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è Screen time JSON parse error: {e}\")\n",
        "            print(f\"Response: {response}\")\n",
        "            return {\n",
        "                \"clarity_score\": 50,\n",
        "                \"fatigue\": \"Analysis error\",\n",
        "                \"avoid_apps\": [],\n",
        "                \"recommend\": {\"movies\": [], \"songs\": []},\n",
        "                \"habits\": []\n",
        "            }\n",
        "\n",
        "    async def analyze_tweet_sentiment(self, tweet: str) -> str:\n",
        "        \"\"\"Analyze sentiment of a single tweet using MCP\"\"\"\n",
        "        prompt = f'Tweet: \"{tweet}\"\\nClassify as one word: Positive, Negative, or Neutral.'\n",
        "\n",
        "        async with MCPLLMClient(self.mcp_server_command) as llm:\n",
        "            response = await llm.generate_response(prompt, max_tokens=10)\n",
        "\n",
        "        sentiment = response.strip().capitalize()\n",
        "        return sentiment if sentiment in [\"Positive\", \"Negative\", \"Neutral\"] else \"Neutral\"\n",
        "\n",
        "    async def analyze_tweets(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Analyze Twitter sentiment using MCP\"\"\"\n",
        "        tweet_col = next(\n",
        "            (col for col in df.columns if col.lower() in [\"tweet\", \"text\", \"message\", \"content\"]),\n",
        "            None\n",
        "        )\n",
        "        if not tweet_col:\n",
        "            tweet_col = df.select_dtypes(include='object').apply(\n",
        "                lambda c: c.str.len().mean()\n",
        "            ).idxmax()\n",
        "\n",
        "        sentiments = []\n",
        "        for tweet in tqdm(df[tweet_col], desc=\"Analyzing tweet sentiments\"):\n",
        "            try:\n",
        "                sentiment = await self.analyze_tweet_sentiment(str(tweet))\n",
        "                sentiments.append(sentiment)\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing tweet: {e}\")\n",
        "                sentiments.append(\"Error\")\n",
        "\n",
        "        df[\"sentiment\"] = sentiments\n",
        "        return df\n",
        "\n",
        "    async def synthesize_report(\n",
        "        self,\n",
        "        chat_result: Dict[str, Any],\n",
        "        screen_result: Dict[str, Any],\n",
        "        sentiment_df: pd.DataFrame\n",
        "    ) -> str:\n",
        "        \"\"\"Generate final mental health report using MCP\"\"\"\n",
        "        sentiment_summary = sentiment_df[\"sentiment\"].value_counts().to_dict()\n",
        "\n",
        "        prompt = f\"\"\"You are a NeuroAI advisor.\n",
        "\n",
        "Combine:\n",
        "1. WhatsApp analysis: {json.dumps(chat_result, indent=2)}\n",
        "2. Screen time report: {json.dumps(screen_result, indent=2)}\n",
        "3. Twitter sentiment: {sentiment_summary}\n",
        "\n",
        "Summarize teen mental health:\n",
        "- Mood and stress\n",
        "- Top 3 issues\n",
        "- Mindful movie/song list\n",
        "- 3 futuristic daily mental health habits\n",
        "\n",
        "Respond in natural tone.\"\"\"\n",
        "\n",
        "        async with MCPLLMClient(self.mcp_server_command) as llm:\n",
        "            return await llm.generate_response(prompt, max_tokens=3000)\n",
        "\n",
        "    async def run_analysis(\n",
        "        self,\n",
        "        whatsapp_file: str,\n",
        "        screen_time_file: str,\n",
        "        twitter_file: str\n",
        "    ) -> str:\n",
        "        \"\"\"Run complete mental health analysis pipeline\"\"\"\n",
        "        try:\n",
        "            # Extract and analyze WhatsApp messages\n",
        "            print(\"üì± Extracting WhatsApp messages...\")\n",
        "            chat_msgs = await self.extract_whatsapp_messages(whatsapp_file)\n",
        "            chat_result = await self.analyze_chat(chat_msgs) if chat_msgs else {\"error\": \"No messages\"}\n",
        "\n",
        "            # Load and analyze screen time data\n",
        "            print(\"üìä Analyzing screen time...\")\n",
        "            screen_df = self.load_screen_time(screen_time_file)\n",
        "            screen_result = await self.analyze_screen_time(screen_df)\n",
        "\n",
        "            # Load and analyze Twitter data\n",
        "            print(\"üê¶ Analyzing Twitter sentiment...\")\n",
        "            tweets_df = pd.read_csv(twitter_file)\n",
        "            sentiment_df = await self.analyze_tweets(tweets_df)\n",
        "\n",
        "            # Generate final report\n",
        "            print(\"üß† Synthesizing final report...\")\n",
        "            final_report = await self.synthesize_report(chat_result, screen_result, sentiment_df)\n",
        "            return final_report\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Pipeline Error: {e}\"\n",
        "\n",
        "# --- Configuration and Usage ---\n",
        "async def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "\n",
        "    # MCP server configuration - adjust based on your MCP setup\n",
        "    # Example configurations for different MCP servers:\n",
        "\n",
        "    # For Claude MCP server\n",
        "    mcp_server_command = [\"claude-mcp-server\", \"--model\", \"claude-3-sonnet\"]\n",
        "\n",
        "    # For OpenAI MCP server\n",
        "    # mcp_server_command = [\"openai-mcp-server\", \"--model\", \"gpt-4\"]\n",
        "\n",
        "    # For local MCP server\n",
        "    # mcp_server_command = [\"python\", \"local_mcp_server.py\"]\n",
        "\n",
        "    # File paths\n",
        "    whatsapp_file = \"whatsapp_chat_analysis.zip\"\n",
        "    screen_time_file = \"screentime_analysis.csv\"\n",
        "    twitter_file = \"teen_tweets.csv\"\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = MentalHealthAnalyzer(mcp_server_command)\n",
        "\n",
        "    # Run analysis\n",
        "    print(\"üöÄ Starting MCP-powered mental health analysis...\")\n",
        "    final_report = await analyzer.run_analysis(whatsapp_file, screen_time_file, twitter_file)\n",
        "\n",
        "    print(\"\\nüß† Final Teen Mental Health Summary:\\n\")\n",
        "    print(final_report)\n",
        "\n",
        "# Alternative synchronous wrapper for easier usage\n",
        "def run_analysis_sync():\n",
        "    \"\"\"Synchronous wrapper for the async analysis\"\"\"\n",
        "    return asyncio.run(main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the analysis\n",
        "     asyncio.run(main())\n",
        "\n",
        "    # Or use the sync wrapper:\n",
        "    # run_analysis_sync()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYyAsKqzRxh",
        "outputId": "ac6ff4e9-ab05-4dda-922a-34c5f79a9ac0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting MCP-powered mental health analysis...\n",
            "üì± Extracting WhatsApp messages...\n",
            "‚ùå Error extracting chat: [Errno 2] No such file or directory: 'whatsapp_chat_analysis.zip'\n",
            "üìä Analyzing screen time...\n",
            "‚ùå Failed to load screen time CSV: [Errno 2] No such file or directory: 'screentime_analysis.csv'\n",
            "üê¶ Analyzing Twitter sentiment...\n",
            "\n",
            "üß† Final Teen Mental Health Summary:\n",
            "\n",
            "‚ùå Pipeline Error: [Errno 2] No such file or directory: 'teen_tweets.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mcp_config.py\n",
        "\"\"\"\n",
        "MCP Configuration for Mental Health Analysis\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class MCPConfig:\n",
        "    \"\"\"Configuration manager for different MCP servers\"\"\"\n",
        "\n",
        "    # MCP Server Configurations\n",
        "    SERVERS = {\n",
        "        \"claude\": {\n",
        "            \"command\": [\"claude-mcp-server\"],\n",
        "            \"args\": [\"--model\", \"claude-3-sonnet\"],\n",
        "            \"description\": \"Anthropic Claude MCP Server\"\n",
        "        },\n",
        "        \"openai\": {\n",
        "            \"command\": [\"openai-mcp-server\"],\n",
        "            \"args\": [\"--model\", \"gpt-4\"],\n",
        "            \"description\": \"OpenAI MCP Server\"\n",
        "        },\n",
        "        \"local\": {\n",
        "            \"command\": [\"python\", \"local_mcp_server.py\"],\n",
        "            \"args\": [],\n",
        "            \"description\": \"Local MCP Server\"\n",
        "        },\n",
        "        \"ollama\": {\n",
        "            \"command\": [\"ollama-mcp-server\"],\n",
        "            \"args\": [\"--model\", \"llama3\"],\n",
        "            \"description\": \"Ollama Local MCP Server\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_server_command(cls, server_name: str) -> List[str]:\n",
        "        \"\"\"Get MCP server command for specified server\"\"\"\n",
        "        if server_name not in cls.SERVERS:\n",
        "            raise ValueError(f\"Unknown server: {server_name}. Available: {list(cls.SERVERS.keys())}\")\n",
        "\n",
        "        config = cls.SERVERS[server_name]\n",
        "        return config[\"command\"] + config[\"args\"]\n",
        "\n",
        "    @classmethod\n",
        "    def list_servers(cls) -> Dict[str, str]:\n",
        "        \"\"\"List available MCP servers\"\"\"\n",
        "        return {name: config[\"description\"] for name, config in cls.SERVERS.items()}\n",
        "\n",
        "# Environment setup\n",
        "def setup_environment():\n",
        "    \"\"\"Setup environment variables for MCP\"\"\"\n",
        "    # Load from .env file if it exists\n",
        "    env_vars = {\n",
        "        \"ANTHROPIC_API_KEY\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
        "        \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "        \"MCP_SERVER_PATH\": os.getenv(\"MCP_SERVER_PATH\", \"./mcp_servers/\"),\n",
        "        \"MCP_LOG_LEVEL\": os.getenv(\"MCP_LOG_LEVEL\", \"INFO\")\n",
        "    }\n",
        "\n",
        "    return env_vars\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Available MCP Servers:\")\n",
        "    for name, desc in MCPConfig.list_servers().items():\n",
        "        print(f\"  {name}: {desc}\")\n",
        "        print(f\"    Command: {' '.join(MCPConfig.get_server_command(name))}\")\n",
        "        print()\n",
        "\n",
        "    print(\"Environment Variables:\")\n",
        "    env = setup_environment()\n",
        "    for key, value in env.items():\n",
        "        print(f\"  {key}: {'SET' if value else 'NOT SET'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIA4Wlzhz8k_",
        "outputId": "6ee84c9b-bedd-4aa6-8963-788a67976e80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available MCP Servers:\n",
            "  claude: Anthropic Claude MCP Server\n",
            "    Command: claude-mcp-server --model claude-3-sonnet\n",
            "\n",
            "  openai: OpenAI MCP Server\n",
            "    Command: openai-mcp-server --model gpt-4\n",
            "\n",
            "  local: Local MCP Server\n",
            "    Command: python local_mcp_server.py\n",
            "\n",
            "  ollama: Ollama Local MCP Server\n",
            "    Command: ollama-mcp-server --model llama3\n",
            "\n",
            "Environment Variables:\n",
            "  ANTHROPIC_API_KEY: NOT SET\n",
            "  OPENAI_API_KEY: NOT SET\n",
            "  MCP_SERVER_PATH: SET\n",
            "  MCP_LOG_LEVEL: SET\n"
          ]
        }
      ]
    }
  ]
}